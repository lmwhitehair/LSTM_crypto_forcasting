{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aba9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61be9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d8736fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      time       low      high      open     close       volume\n",
       "0     2015-11-30 12:00:00    367.50    379.42    378.16    373.00  3031.105717\n",
       "1     2015-11-30 18:00:00    372.25    378.55    373.00    376.86  2652.465161\n",
       "2     2015-12-01 00:00:00    375.80    378.94    376.88    378.01  1695.388592\n",
       "3     2015-12-01 06:00:00    354.60    378.54    378.00    361.20  2682.538466\n",
       "4     2015-12-01 12:00:00    355.00    364.86    361.21    360.14  2864.005011\n",
       "...                   ...       ...       ...       ...       ...          ...\n",
       "11622 2023-11-14 06:00:00  36249.50  36756.16  36642.00  36272.73  1455.611439\n",
       "11623 2023-11-14 12:00:00  35855.00  36708.43  36272.63  36088.57  5289.680943\n",
       "11624 2023-11-14 18:00:00  34758.64  36122.76  36090.42  35554.09  7511.702326\n",
       "11625 2023-11-15 00:00:00  35358.45  35670.90  35554.10  35610.51  1929.755005\n",
       "11626 2023-11-15 06:00:00  35533.29  36293.22  35610.08  36253.78  1693.626865\n",
       "\n",
       "[11627 rows x 6 columns]>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    return df\n",
    "\n",
    "df = clean(\"train.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3425ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      time       low      high      open     close    volume\n",
       "0     2015-11-30 12:00:00  0.005457  0.000341  0.000368  0.000293  0.029584\n",
       "1     2015-11-30 18:00:00  0.005527  0.000328  0.000293  0.000349  0.025770\n",
       "2     2015-12-01 00:00:00  0.005580  0.000334  0.000350  0.000366  0.016130\n",
       "3     2015-12-01 06:00:00  0.005265  0.000328  0.000366  0.000120  0.026073\n",
       "4     2015-12-01 12:00:00  0.005271  0.000129  0.000120  0.000104  0.027901\n",
       "...                   ...       ...       ...       ...       ...       ...\n",
       "11622 2023-11-14 06:00:00  0.538342  0.530274  0.531431  0.526014  0.013715\n",
       "11623 2023-11-14 12:00:00  0.532483  0.529579  0.526022  0.523317  0.052333\n",
       "11624 2023-11-14 18:00:00  0.516201  0.521047  0.523353  0.515490  0.074715\n",
       "11625 2023-11-15 00:00:00  0.525109  0.514464  0.515499  0.516316  0.018491\n",
       "11626 2023-11-15 06:00:00  0.527706  0.523530  0.516319  0.525737  0.016112\n",
       "\n",
       "[11627 rows x 6 columns]>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = ['low', 'high', 'open', 'close', 'volume']  # adjust with your column names\n",
    "    df[cols] = scaler.fit_transform(df[cols])\n",
    "    return df, scaler\n",
    "\n",
    "def reverse_norm(pred, scaler):\n",
    "    predictions = pred.reshape(-1, 1)\n",
    "    real_pred = scaler.inverse_transform(predictions)\n",
    "    return real_pred\n",
    "    \n",
    "norm, scaler = normalize(df)\n",
    "norm.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9249568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, window_size):\n",
    "    sequences = []\n",
    "    df_size = len(df)\n",
    "    for i in range(df_size - window_size):\n",
    "        # Here, i: start of the sequence, i + window_size: end of the sequence\n",
    "        sequence = df[i:i + window_size]\n",
    "        label = df[i + window_size:i + window_size + 1]  # next value to be predicted\n",
    "        sequences.append((sequence, label))\n",
    "    return sequences\n",
    "\n",
    "window_size = 10  # This is just an example value\n",
    "\n",
    "# Assuming 'close' is what you want to predict\n",
    "sequences = create_sequences(norm['close'].values, window_size)\n",
    "sequences = [(torch.FloatTensor(seq), torch.FloatTensor(lbl)) for seq, lbl in sequences]\n",
    "#print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c70b4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, test_size=0.4, val_size=0.5, random_state=42):\n",
    "    X, y = zip(*sequences)  # No change here, but X and y are already tensors\n",
    "    X = torch.stack(X)  # Stack all sequence tensors\n",
    "    y = torch.stack(y).squeeze()  # Stack all label tensors and remove extra dimension\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = torch.randperm(X.size(0))\n",
    "    X, y = X[indices], y[indices]\n",
    "\n",
    "    # Calculate split sizes\n",
    "    train_end = int(X.size(0) * (1 - test_size))\n",
    "    val_end = int(train_end * (1 - val_size))\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp = X[:train_end], X[train_end:]\n",
    "    y_train, y_temp = y[:train_end], y[train_end:]\n",
    "    X_val, X_test = X_temp[:val_end], X_temp[val_end:]\n",
    "    y_val, y_test = y_temp[:val_end], y_temp[val_end:]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1249c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you have already run the modified split_sequences function\n",
    "# and have X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create TensorDataset\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64  # Adjust as necessary\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8765b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "\n",
    "        # The output layer that maps the hidden state output to the desired output size\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), -1, input_size))\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "num_features = 5\n",
    "input_size = num_features  # number of features in your input\n",
    "hidden_layer_size = 180  # number of features in hidden state\n",
    "output_size = 1  # predict one feature\n",
    "\n",
    "model = LSTM(input_size, hidden_layer_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a56a66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # Setting bidirectional=True doubles the output feature dimension \n",
    "        # because it concatenates the hidden states from both directions\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Since the LSTM is bidirectional, we need to double the input feature dimension\n",
    "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # lstm_out shape is (batch, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), -1, input_size))\n",
    "        \n",
    "        # We take the last time step's output from both directions\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "#hidden_layer_size = 180  # number of features in hidden state\n",
    "\n",
    "bi_model = BiLSTM(input_size, hidden_layer_size, output_size)\n",
    "\n",
    "bi_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c48424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.0316158012 val_loss: 0.0620016681\n",
      "Epoch 2 train_loss: 0.0069178897 val_loss: 0.0821871558\n",
      "Epoch 3 train_loss: 0.0005344134 val_loss: 0.1052439942\n",
      "Epoch 4 train_loss: 0.0004439712 val_loss: 0.1118299652\n",
      "Epoch 5 train_loss: 0.0002851372 val_loss: 0.1123840959\n",
      "Epoch 6 train_loss: 0.0002024353 val_loss: 0.1125983956\n",
      "Epoch 7 train_loss: 0.0002543227 val_loss: 0.1126371359\n",
      "Epoch 8 train_loss: 0.0003676412 val_loss: 0.1127246758\n",
      "Epoch 9 train_loss: 0.0002416756 val_loss: 0.1128042874\n",
      "Epoch 10 train_loss: 0.0000852508 val_loss: 0.1126651916\n",
      "Epoch 11 train_loss: 0.0002522433 val_loss: 0.1125624926\n",
      "Epoch 12 train_loss: 0.0002764882 val_loss: 0.1127866089\n",
      "Epoch 13 train_loss: 0.0001995529 val_loss: 0.1130222933\n",
      "Epoch 14 train_loss: 0.0003042336 val_loss: 0.1126736757\n",
      "Epoch 15 train_loss: 0.0002719340 val_loss: 0.1128663283\n",
      "Epoch 16 train_loss: 0.0000792651 val_loss: 0.1125529106\n",
      "Epoch 17 train_loss: 0.0002729457 val_loss: 0.1130317332\n",
      "Epoch 18 train_loss: 0.0001249508 val_loss: 0.1127112463\n",
      "Epoch 19 train_loss: 0.0001058953 val_loss: 0.1127708273\n",
      "Epoch 20 train_loss: 0.0001388212 val_loss: 0.1125498560\n",
      "Epoch 21 train_loss: 0.0001313730 val_loss: 0.1126214736\n",
      "Epoch 22 train_loss: 0.0000692911 val_loss: 0.1131011085\n",
      "Epoch 23 train_loss: 0.0002952715 val_loss: 0.1123814843\n",
      "Epoch 24 train_loss: 0.0001878936 val_loss: 0.1127906940\n",
      "Epoch 25 train_loss: 0.0002098714 val_loss: 0.1128711279\n",
      "Epoch 26 train_loss: 0.0001021529 val_loss: 0.1129010706\n",
      "Epoch 27 train_loss: 0.0001186389 val_loss: 0.1122974388\n",
      "Epoch 28 train_loss: 0.0000777489 val_loss: 0.1133140703\n",
      "Epoch 29 train_loss: 0.0000917408 val_loss: 0.1127223538\n",
      "Epoch 30 train_loss: 0.0001790249 val_loss: 0.1130082773\n",
      "Epoch 31 train_loss: 0.0000582775 val_loss: 0.1123824741\n",
      "Epoch 32 train_loss: 0.0001184888 val_loss: 0.1120567945\n",
      "Epoch 33 train_loss: 0.0002000801 val_loss: 0.1124223534\n",
      "Epoch 34 train_loss: 0.0001251745 val_loss: 0.1129580587\n",
      "Epoch 35 train_loss: 0.0002750878 val_loss: 0.1127551970\n",
      "Epoch 36 train_loss: 0.0002152593 val_loss: 0.1126774864\n",
      "Epoch 37 train_loss: 0.0001965467 val_loss: 0.1128617688\n",
      "Epoch 38 train_loss: 0.0002255127 val_loss: 0.1125292416\n",
      "Epoch 39 train_loss: 0.0002934340 val_loss: 0.1120642161\n",
      "Epoch 40 train_loss: 0.0001995774 val_loss: 0.1135329081\n",
      "Epoch 41 train_loss: 0.0002221251 val_loss: 0.1132511028\n",
      "Epoch 42 train_loss: 0.0001817207 val_loss: 0.1131428666\n",
      "Epoch 43 train_loss: 0.0001445452 val_loss: 0.1130729902\n",
      "Epoch 44 train_loss: 0.0002356673 val_loss: 0.1124728543\n",
      "Epoch 45 train_loss: 0.0000949464 val_loss: 0.1133493778\n",
      "Epoch 46 train_loss: 0.0000503355 val_loss: 0.1119360497\n",
      "Epoch 47 train_loss: 0.0002369963 val_loss: 0.1117023020\n",
      "Epoch 48 train_loss: 0.0001546346 val_loss: 0.1126745070\n",
      "Epoch 49 train_loss: 0.0001065872 val_loss: 0.1128892182\n",
      "Epoch 50 train_loss: 0.0002036089 val_loss: 0.1125910302\n",
      "Epoch 51 train_loss: 0.0001187139 val_loss: 0.1133358301\n",
      "Epoch 52 train_loss: 0.0001693037 val_loss: 0.1131500275\n",
      "Epoch 53 train_loss: 0.0001537391 val_loss: 0.1125844701\n",
      "Epoch 54 train_loss: 0.0002127215 val_loss: 0.1139966993\n",
      "Epoch 55 train_loss: 0.0002007726 val_loss: 0.1124514815\n",
      "Epoch 56 train_loss: 0.0001557779 val_loss: 0.1122256401\n",
      "Epoch 57 train_loss: 0.0001859433 val_loss: 0.1129145149\n",
      "Epoch 58 train_loss: 0.0001396170 val_loss: 0.1128773242\n",
      "Epoch 59 train_loss: 0.0001586699 val_loss: 0.1124189779\n",
      "Epoch 60 train_loss: 0.0000635183 val_loss: 0.1128075741\n",
      "Epoch 61 train_loss: 0.0000838197 val_loss: 0.1126368329\n",
      "Epoch 62 train_loss: 0.0000859099 val_loss: 0.1128976704\n",
      "Epoch 63 train_loss: 0.0000631021 val_loss: 0.1128016025\n",
      "Epoch 64 train_loss: 0.0000587261 val_loss: 0.1126910987\n",
      "Epoch 65 train_loss: 0.0000716112 val_loss: 0.1125523556\n",
      "Epoch 66 train_loss: 0.0000563014 val_loss: 0.1118767137\n",
      "Epoch 67 train_loss: 0.0000605719 val_loss: 0.1128738061\n",
      "Epoch 68 train_loss: 0.0001023594 val_loss: 0.1126355488\n",
      "Epoch 69 train_loss: 0.0001414990 val_loss: 0.1129075073\n",
      "Epoch 70 train_loss: 0.0002379342 val_loss: 0.1119148619\n",
      "Epoch 71 train_loss: 0.0001615495 val_loss: 0.1123664527\n",
      "Epoch 72 train_loss: 0.0000892541 val_loss: 0.1125365168\n",
      "Epoch 73 train_loss: 0.0000750360 val_loss: 0.1134961563\n",
      "Epoch 74 train_loss: 0.0000834885 val_loss: 0.1126867017\n",
      "Epoch 75 train_loss: 0.0000815231 val_loss: 0.1127678602\n",
      "Epoch 76 train_loss: 0.0001141679 val_loss: 0.1125067769\n",
      "Epoch 77 train_loss: 0.0001272559 val_loss: 0.1122374096\n",
      "Epoch 78 train_loss: 0.0000814827 val_loss: 0.1122980372\n",
      "Epoch 79 train_loss: 0.0000574546 val_loss: 0.1124037690\n",
      "Epoch 80 train_loss: 0.0000592445 val_loss: 0.1123975985\n",
      "Epoch 81 train_loss: 0.0000476298 val_loss: 0.1131806550\n",
      "Epoch 82 train_loss: 0.0000650765 val_loss: 0.1130417396\n",
      "Epoch 83 train_loss: 0.0001606947 val_loss: 0.1130594641\n",
      "Epoch 84 train_loss: 0.0001097896 val_loss: 0.1129740107\n",
      "Epoch 85 train_loss: 0.0000579659 val_loss: 0.1134298540\n",
      "Epoch 86 train_loss: 0.0000449321 val_loss: 0.1127905011\n",
      "Epoch 87 train_loss: 0.0000537231 val_loss: 0.1122865014\n",
      "Epoch 88 train_loss: 0.0000926426 val_loss: 0.1115497574\n",
      "Epoch 89 train_loss: 0.0001021175 val_loss: 0.1127674939\n",
      "Epoch 90 train_loss: 0.0000948528 val_loss: 0.1130927236\n",
      "Epoch 91 train_loss: 0.0000989055 val_loss: 0.1126145588\n",
      "Epoch 92 train_loss: 0.0000520445 val_loss: 0.1129421833\n",
      "Epoch 93 train_loss: 0.0002014593 val_loss: 0.1124870246\n",
      "Epoch 94 train_loss: 0.0000330528 val_loss: 0.1133688304\n",
      "Epoch 95 train_loss: 0.0000736940 val_loss: 0.1135638314\n",
      "Epoch 96 train_loss: 0.0000586177 val_loss: 0.1124432086\n",
      "Epoch 97 train_loss: 0.0005044865 val_loss: 0.1126759643\n",
      "Epoch 98 train_loss: 0.0000338256 val_loss: 0.1131620211\n",
      "Epoch 99 train_loss: 0.0000339393 val_loss: 0.1129203307\n",
      "Epoch 100 train_loss: 0.0000442592 val_loss: 0.1136969146\n",
      "Epoch 101 train_loss: 0.0000491706 val_loss: 0.1130600471\n",
      "Epoch 102 train_loss: 0.0000702455 val_loss: 0.1133216055\n",
      "Epoch 103 train_loss: 0.0000300710 val_loss: 0.1134306905\n",
      "Epoch 104 train_loss: 0.0000416269 val_loss: 0.1129205967\n",
      "Epoch 105 train_loss: 0.0001099941 val_loss: 0.1123127080\n",
      "Epoch 106 train_loss: 0.0000308082 val_loss: 0.1119039744\n",
      "Epoch 107 train_loss: 0.0000718776 val_loss: 0.1126063385\n",
      "Epoch 108 train_loss: 0.0001117378 val_loss: 0.1127473125\n",
      "Epoch 109 train_loss: 0.0000689053 val_loss: 0.1131069271\n",
      "Epoch 110 train_loss: 0.0000744435 val_loss: 0.1128072231\n",
      "Epoch 111 train_loss: 0.0000646215 val_loss: 0.1130589661\n",
      "Epoch 112 train_loss: 0.0001419407 val_loss: 0.1135661339\n",
      "Epoch 113 train_loss: 0.0001181963 val_loss: 0.1132862072\n",
      "Epoch 114 train_loss: 0.0000307916 val_loss: 0.1128396218\n",
      "Epoch 115 train_loss: 0.0000508532 val_loss: 0.1126193744\n",
      "Epoch 116 train_loss: 0.0001952239 val_loss: 0.1131140247\n",
      "Epoch 117 train_loss: 0.0000738211 val_loss: 0.1129058215\n",
      "Epoch 118 train_loss: 0.0000472871 val_loss: 0.1133422261\n",
      "Epoch 119 train_loss: 0.0000594729 val_loss: 0.1129822222\n",
      "Epoch 120 train_loss: 0.0000487216 val_loss: 0.1136538450\n",
      "Epoch 121 train_loss: 0.0000657738 val_loss: 0.1123608069\n",
      "Epoch 122 train_loss: 0.0000627662 val_loss: 0.1122128802\n",
      "Epoch 123 train_loss: 0.0000465690 val_loss: 0.1134687241\n",
      "Epoch 124 train_loss: 0.0000205614 val_loss: 0.1131082263\n",
      "Epoch 125 train_loss: 0.0000372322 val_loss: 0.1137827693\n",
      "Epoch 126 train_loss: 0.0000745466 val_loss: 0.1132527821\n",
      "Epoch 127 train_loss: 0.0001201504 val_loss: 0.1127379939\n",
      "Epoch 128 train_loss: 0.0000376634 val_loss: 0.1129973475\n",
      "Epoch 129 train_loss: 0.0001632211 val_loss: 0.1119881889\n",
      "Epoch 130 train_loss: 0.0000239023 val_loss: 0.1133883432\n",
      "Epoch 131 train_loss: 0.0000360204 val_loss: 0.1126247739\n",
      "Epoch 132 train_loss: 0.0000300671 val_loss: 0.1131446321\n",
      "Epoch 133 train_loss: 0.0000292359 val_loss: 0.1123576698\n",
      "Epoch 134 train_loss: 0.0000325713 val_loss: 0.1124424884\n",
      "Epoch 135 train_loss: 0.0000749838 val_loss: 0.1129598626\n",
      "Epoch 136 train_loss: 0.0000466476 val_loss: 0.1123177428\n",
      "Epoch 137 train_loss: 0.0000213316 val_loss: 0.1121952433\n",
      "Epoch 138 train_loss: 0.0000719142 val_loss: 0.1128248971\n",
      "Epoch 139 train_loss: 0.0000483246 val_loss: 0.1135987890\n",
      "Epoch 140 train_loss: 0.0000580618 val_loss: 0.1130941073\n",
      "Epoch 141 train_loss: 0.0000844244 val_loss: 0.1133279693\n",
      "Epoch 142 train_loss: 0.0000512758 val_loss: 0.1131671324\n",
      "Epoch 143 train_loss: 0.0000330314 val_loss: 0.1125895365\n",
      "Epoch 144 train_loss: 0.0000350844 val_loss: 0.1128554668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 train_loss: 0.0001071306 val_loss: 0.1125924102\n",
      "Epoch 146 train_loss: 0.0000389395 val_loss: 0.1134597375\n",
      "Epoch 147 train_loss: 0.0001117307 val_loss: 0.1134924374\n",
      "Epoch 148 train_loss: 0.0000494746 val_loss: 0.1133075263\n",
      "Epoch 149 train_loss: 0.0000492210 val_loss: 0.1136605695\n",
      "Epoch 150 train_loss: 0.0000358955 val_loss: 0.1127418847\n",
      "Epoch 151 train_loss: 0.0000460372 val_loss: 0.1130085929\n",
      "Epoch 152 train_loss: 0.0000738064 val_loss: 0.1135736992\n",
      "Epoch 153 train_loss: 0.0000548319 val_loss: 0.1141210496\n",
      "Epoch 154 train_loss: 0.0000872358 val_loss: 0.1119706270\n",
      "Epoch 155 train_loss: 0.0000371082 val_loss: 0.1125231313\n",
      "Epoch 156 train_loss: 0.0000231411 val_loss: 0.1130766126\n",
      "Epoch 157 train_loss: 0.0000329636 val_loss: 0.1138247960\n",
      "Epoch 158 train_loss: 0.0000732225 val_loss: 0.1135207990\n",
      "Epoch 159 train_loss: 0.0000596370 val_loss: 0.1128636194\n",
      "Epoch 160 train_loss: 0.0000408327 val_loss: 0.1134204657\n",
      "Epoch 161 train_loss: 0.0001706966 val_loss: 0.1129085190\n",
      "Epoch 162 train_loss: 0.0000342191 val_loss: 0.1132521623\n",
      "Epoch 163 train_loss: 0.0000273552 val_loss: 0.1132088103\n",
      "Epoch 164 train_loss: 0.0000361760 val_loss: 0.1127592148\n",
      "Epoch 165 train_loss: 0.0000226574 val_loss: 0.1127820352\n",
      "Epoch 166 train_loss: 0.0000277243 val_loss: 0.1136237001\n",
      "Epoch 167 train_loss: 0.0000231194 val_loss: 0.1127494751\n",
      "Epoch 168 train_loss: 0.0000277842 val_loss: 0.1135277254\n",
      "Epoch 169 train_loss: 0.0001289319 val_loss: 0.1130757169\n",
      "Epoch 170 train_loss: 0.0000210767 val_loss: 0.1125836080\n",
      "Epoch 171 train_loss: 0.0000140573 val_loss: 0.1122676974\n",
      "Epoch 172 train_loss: 0.0000209104 val_loss: 0.1134093847\n",
      "Epoch 173 train_loss: 0.0000799322 val_loss: 0.1132590385\n",
      "Epoch 174 train_loss: 0.0000400468 val_loss: 0.1122645631\n",
      "Epoch 175 train_loss: 0.0001391960 val_loss: 0.1133098805\n",
      "Epoch 176 train_loss: 0.0000294039 val_loss: 0.1131313189\n",
      "Epoch 177 train_loss: 0.0000254720 val_loss: 0.1136497629\n",
      "Epoch 178 train_loss: 0.0000408194 val_loss: 0.1132186657\n",
      "Epoch 179 train_loss: 0.0000215029 val_loss: 0.1127674547\n",
      "Epoch 180 train_loss: 0.0001834252 val_loss: 0.1127063896\n",
      "Epoch 181 train_loss: 0.0000275939 val_loss: 0.1130238467\n",
      "Epoch 182 train_loss: 0.0000732499 val_loss: 0.1132664242\n",
      "Epoch 183 train_loss: 0.0000304345 val_loss: 0.1131368447\n",
      "Epoch 184 train_loss: 0.0000253644 val_loss: 0.1133508288\n",
      "Epoch 185 train_loss: 0.0000295316 val_loss: 0.1123949659\n",
      "Epoch 186 train_loss: 0.0000302016 val_loss: 0.1126600378\n",
      "Epoch 187 train_loss: 0.0000430837 val_loss: 0.1131055157\n",
      "Epoch 188 train_loss: 0.0000440298 val_loss: 0.1132618269\n",
      "Epoch 189 train_loss: 0.0000858899 val_loss: 0.1123234309\n",
      "Epoch 190 train_loss: 0.0000231055 val_loss: 0.1127387169\n",
      "Epoch 191 train_loss: 0.0000827716 val_loss: 0.1130669239\n",
      "Epoch 192 train_loss: 0.0001947143 val_loss: 0.1129864141\n",
      "Epoch 193 train_loss: 0.0000549764 val_loss: 0.1136302831\n",
      "Epoch 194 train_loss: 0.0000394311 val_loss: 0.1130546231\n",
      "Epoch 195 train_loss: 0.0000654727 val_loss: 0.1133391593\n",
      "Epoch 196 train_loss: 0.0000229765 val_loss: 0.1128902338\n",
      "Epoch 197 train_loss: 0.0000416506 val_loss: 0.1128179785\n",
      "Epoch 198 train_loss: 0.0000813649 val_loss: 0.1128328753\n",
      "Epoch 199 train_loss: 0.0000452523 val_loss: 0.1123972644\n",
      "Epoch 200 train_loss: 0.0000289123 val_loss: 0.1138043730\n",
      "Epoch 201 train_loss: 0.0000314916 val_loss: 0.1129933085\n",
      "Epoch 202 train_loss: 0.0000213841 val_loss: 0.1129774774\n",
      "Epoch 203 train_loss: 0.0000402154 val_loss: 0.1131636736\n",
      "Epoch 204 train_loss: 0.0000285028 val_loss: 0.1124106940\n",
      "Epoch 205 train_loss: 0.0000467785 val_loss: 0.1135047677\n",
      "Epoch 206 train_loss: 0.0000269915 val_loss: 0.1126133336\n",
      "Epoch 207 train_loss: 0.0001164835 val_loss: 0.1128166581\n",
      "Epoch 208 train_loss: 0.0000302947 val_loss: 0.1129922437\n",
      "Epoch 209 train_loss: 0.0001901558 val_loss: 0.1123717093\n",
      "Epoch 210 train_loss: 0.0000225430 val_loss: 0.1133362590\n",
      "Epoch 211 train_loss: 0.0000952919 val_loss: 0.1127044930\n",
      "Epoch 212 train_loss: 0.0000277544 val_loss: 0.1125663519\n",
      "Epoch 213 train_loss: 0.0000213210 val_loss: 0.1128953055\n",
      "Epoch 214 train_loss: 0.0000759072 val_loss: 0.1132297410\n",
      "Epoch 215 train_loss: 0.0000538254 val_loss: 0.1132591157\n",
      "Epoch 216 train_loss: 0.0001069831 val_loss: 0.1135390902\n",
      "Epoch 217 train_loss: 0.0000457534 val_loss: 0.1133134771\n",
      "Epoch 218 train_loss: 0.0000262788 val_loss: 0.1132246325\n",
      "Epoch 219 train_loss: 0.0000278497 val_loss: 0.1128428826\n",
      "Epoch 220 train_loss: 0.0000280270 val_loss: 0.1128305718\n",
      "Epoch 221 train_loss: 0.0001016400 val_loss: 0.1137772797\n",
      "Epoch 222 train_loss: 0.0000191334 val_loss: 0.1137188519\n",
      "Epoch 223 train_loss: 0.0000505364 val_loss: 0.1130383383\n",
      "Epoch 224 train_loss: 0.0001008487 val_loss: 0.1129528760\n",
      "Epoch 225 train_loss: 0.0000311966 val_loss: 0.1125879057\n",
      "Epoch 226 train_loss: 0.0000986932 val_loss: 0.1126571640\n",
      "Epoch 227 train_loss: 0.0000322150 val_loss: 0.1136920953\n",
      "Epoch 228 train_loss: 0.0000159614 val_loss: 0.1126344563\n",
      "Epoch 229 train_loss: 0.0000227088 val_loss: 0.1130929839\n",
      "Epoch 230 train_loss: 0.0000228974 val_loss: 0.1124669216\n",
      "Epoch 231 train_loss: 0.0000319348 val_loss: 0.1129769342\n",
      "Epoch 232 train_loss: 0.0000721372 val_loss: 0.1136139759\n",
      "Epoch 233 train_loss: 0.0000225378 val_loss: 0.1124301019\n",
      "Epoch 234 train_loss: 0.0000680015 val_loss: 0.1125709974\n",
      "Epoch 235 train_loss: 0.0001640755 val_loss: 0.1129015757\n",
      "Epoch 236 train_loss: 0.0000612149 val_loss: 0.1127226435\n",
      "Epoch 237 train_loss: 0.0000623965 val_loss: 0.1131821119\n",
      "Epoch 238 train_loss: 0.0000121634 val_loss: 0.1130086453\n",
      "Epoch 239 train_loss: 0.0000223225 val_loss: 0.1130476994\n",
      "Epoch 240 train_loss: 0.0000291275 val_loss: 0.1127427697\n",
      "Epoch 241 train_loss: 0.0001028957 val_loss: 0.1141640945\n",
      "Epoch 242 train_loss: 0.0000179820 val_loss: 0.1135459775\n",
      "Epoch 243 train_loss: 0.0000543700 val_loss: 0.1132177418\n",
      "Epoch 244 train_loss: 0.0000656078 val_loss: 0.1129194615\n",
      "Epoch 245 train_loss: 0.0000172568 val_loss: 0.1124984289\n",
      "Epoch 246 train_loss: 0.0000272526 val_loss: 0.1135469592\n",
      "Epoch 247 train_loss: 0.0000343253 val_loss: 0.1134495448\n",
      "Epoch 248 train_loss: 0.0000851018 val_loss: 0.1130902175\n",
      "Epoch 249 train_loss: 0.0000182457 val_loss: 0.1127260014\n",
      "Epoch 250 train_loss: 0.0000832908 val_loss: 0.1130536241\n",
      "Epoch 251 train_loss: 0.0000604851 val_loss: 0.1131102153\n",
      "Epoch 252 train_loss: 0.0000075342 val_loss: 0.1130550699\n",
      "Epoch 253 train_loss: 0.0000868548 val_loss: 0.1124768212\n",
      "Epoch 254 train_loss: 0.0000369512 val_loss: 0.1135827128\n",
      "Epoch 255 train_loss: 0.0000216701 val_loss: 0.1130472507\n",
      "Epoch 256 train_loss: 0.0000211337 val_loss: 0.1126808534\n",
      "Epoch 257 train_loss: 0.0000094428 val_loss: 0.1127150728\n",
      "Epoch 258 train_loss: 0.0000276614 val_loss: 0.1128812892\n",
      "Epoch 259 train_loss: 0.0000205444 val_loss: 0.1134195280\n",
      "Epoch 260 train_loss: 0.0000269545 val_loss: 0.1129814138\n",
      "Epoch 261 train_loss: 0.0000711186 val_loss: 0.1128970856\n",
      "Epoch 262 train_loss: 0.0000485413 val_loss: 0.1121693711\n",
      "Epoch 263 train_loss: 0.0000509867 val_loss: 0.1127643797\n",
      "Epoch 264 train_loss: 0.0000319458 val_loss: 0.1133109731\n",
      "Epoch 265 train_loss: 0.0000310283 val_loss: 0.1126926697\n",
      "Epoch 266 train_loss: 0.0000566951 val_loss: 0.1132904864\n",
      "Epoch 267 train_loss: 0.0000318310 val_loss: 0.1131328580\n",
      "Epoch 268 train_loss: 0.0000343283 val_loss: 0.1135522700\n",
      "Epoch 269 train_loss: 0.0000429323 val_loss: 0.1131856630\n",
      "Epoch 270 train_loss: 0.0000334621 val_loss: 0.1126658744\n",
      "Epoch 271 train_loss: 0.0000249199 val_loss: 0.1127550411\n",
      "Epoch 272 train_loss: 0.0000404652 val_loss: 0.1131875690\n",
      "Epoch 273 train_loss: 0.0000405639 val_loss: 0.1136036735\n",
      "Epoch 274 train_loss: 0.0000303746 val_loss: 0.1134983334\n",
      "Epoch 275 train_loss: 0.0000312269 val_loss: 0.1127749954\n",
      "Epoch 276 train_loss: 0.0000413212 val_loss: 0.1134968588\n",
      "Epoch 277 train_loss: 0.0000333042 val_loss: 0.1133999559\n",
      "Epoch 278 train_loss: 0.0000347886 val_loss: 0.1129550836\n",
      "Epoch 279 train_loss: 0.0000976327 val_loss: 0.1122516754\n",
      "Epoch 280 train_loss: 0.0000981877 val_loss: 0.1137714591\n",
      "Epoch 281 train_loss: 0.0000505780 val_loss: 0.1128700059\n",
      "Epoch 282 train_loss: 0.0000296828 val_loss: 0.1130656450\n",
      "Epoch 283 train_loss: 0.0000218001 val_loss: 0.1129190020\n",
      "Epoch 284 train_loss: 0.0000158954 val_loss: 0.1130174805\n",
      "Epoch 285 train_loss: 0.0001298617 val_loss: 0.1126694400\n",
      "Epoch 286 train_loss: 0.0001221510 val_loss: 0.1127395206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287 train_loss: 0.0000794272 val_loss: 0.1128388962\n",
      "Epoch 288 train_loss: 0.0000587468 val_loss: 0.1119338992\n",
      "Epoch 289 train_loss: 0.0000255186 val_loss: 0.1130715600\n",
      "Epoch 290 train_loss: 0.0000266363 val_loss: 0.1135104534\n",
      "Epoch 291 train_loss: 0.0000283974 val_loss: 0.1127764969\n",
      "Epoch 292 train_loss: 0.0000407512 val_loss: 0.1130050436\n",
      "Epoch 293 train_loss: 0.0000301068 val_loss: 0.1130920696\n",
      "Epoch 294 train_loss: 0.0000534258 val_loss: 0.1136867010\n",
      "Epoch 295 train_loss: 0.0001449427 val_loss: 0.1131442302\n",
      "Epoch 296 train_loss: 0.0000395840 val_loss: 0.1136048872\n",
      "Epoch 297 train_loss: 0.0000597724 val_loss: 0.1137127077\n",
      "Epoch 298 train_loss: 0.0000829167 val_loss: 0.1128329311\n",
      "Epoch 299 train_loss: 0.0000175883 val_loss: 0.1127230283\n",
      "Epoch 300 train_loss: 0.0000348516 val_loss: 0.1132898267\n",
      "Epoch 301 train_loss: 0.0000361573 val_loss: 0.1130856553\n",
      "Epoch 302 train_loss: 0.0000645093 val_loss: 0.1126067233\n",
      "Epoch 303 train_loss: 0.0000519385 val_loss: 0.1131906986\n",
      "Epoch 304 train_loss: 0.0000589917 val_loss: 0.1133214616\n",
      "Epoch 305 train_loss: 0.0000354199 val_loss: 0.1124886709\n",
      "Epoch 306 train_loss: 0.0002045406 val_loss: 0.1131284628\n",
      "Epoch 307 train_loss: 0.0000346839 val_loss: 0.1131813009\n",
      "Epoch 308 train_loss: 0.0001665520 val_loss: 0.1130663879\n",
      "Epoch 309 train_loss: 0.0000471208 val_loss: 0.1121556562\n",
      "Epoch 310 train_loss: 0.0000319393 val_loss: 0.1139628637\n",
      "Epoch 311 train_loss: 0.0000498096 val_loss: 0.1126119658\n",
      "Epoch 312 train_loss: 0.0000192904 val_loss: 0.1132802847\n",
      "Epoch 313 train_loss: 0.0000295491 val_loss: 0.1137962917\n",
      "Epoch 314 train_loss: 0.0000593916 val_loss: 0.1129208202\n",
      "Epoch 315 train_loss: 0.0001580321 val_loss: 0.1126294630\n",
      "Epoch 316 train_loss: 0.0000474830 val_loss: 0.1126775129\n",
      "Epoch 317 train_loss: 0.0001113935 val_loss: 0.1127188066\n",
      "Epoch 318 train_loss: 0.0000358576 val_loss: 0.1137256458\n",
      "Epoch 319 train_loss: 0.0000550617 val_loss: 0.1131650669\n",
      "Epoch 320 train_loss: 0.0001199080 val_loss: 0.1124081119\n",
      "Epoch 321 train_loss: 0.0000173730 val_loss: 0.1127216903\n",
      "Epoch 322 train_loss: 0.0000492016 val_loss: 0.1131771300\n",
      "Epoch 323 train_loss: 0.0000608885 val_loss: 0.1140377588\n",
      "Epoch 324 train_loss: 0.0000217304 val_loss: 0.1122590171\n",
      "Epoch 325 train_loss: 0.0000733004 val_loss: 0.1129164423\n",
      "Epoch 326 train_loss: 0.0000286412 val_loss: 0.1127362262\n",
      "Epoch 327 train_loss: 0.0000218197 val_loss: 0.1125361489\n",
      "Epoch 328 train_loss: 0.0000395490 val_loss: 0.1135881787\n",
      "Epoch 329 train_loss: 0.0000867158 val_loss: 0.1123381913\n",
      "Epoch 330 train_loss: 0.0001242716 val_loss: 0.1135869659\n",
      "Epoch 331 train_loss: 0.0000409690 val_loss: 0.1125965911\n",
      "Epoch 332 train_loss: 0.0000385428 val_loss: 0.1127113590\n",
      "Epoch 333 train_loss: 0.0000719013 val_loss: 0.1129331272\n",
      "Epoch 334 train_loss: 0.0000222497 val_loss: 0.1129973253\n",
      "Epoch 335 train_loss: 0.0000098732 val_loss: 0.1125178799\n",
      "Epoch 336 train_loss: 0.0000229486 val_loss: 0.1133960893\n",
      "Epoch 337 train_loss: 0.0000434631 val_loss: 0.1135070331\n",
      "Epoch 338 train_loss: 0.0000252904 val_loss: 0.1128457348\n",
      "Epoch 339 train_loss: 0.0000284288 val_loss: 0.1128140110\n",
      "Epoch 340 train_loss: 0.0000340530 val_loss: 0.1130673441\n",
      "Epoch 341 train_loss: 0.0000855227 val_loss: 0.1134656818\n",
      "Epoch 342 train_loss: 0.0001155607 val_loss: 0.1128302389\n",
      "Epoch 343 train_loss: 0.0000775044 val_loss: 0.1126811450\n",
      "Epoch 344 train_loss: 0.0000497446 val_loss: 0.1128075128\n",
      "Epoch 345 train_loss: 0.0000222731 val_loss: 0.1128993984\n",
      "Epoch 346 train_loss: 0.0000666588 val_loss: 0.1129073933\n",
      "Epoch 347 train_loss: 0.0000411548 val_loss: 0.1138566483\n",
      "Epoch 348 train_loss: 0.0000241730 val_loss: 0.1131809077\n",
      "Epoch 349 train_loss: 0.0000764130 val_loss: 0.1129257556\n",
      "Epoch 350 train_loss: 0.0000445508 val_loss: 0.1137379809\n",
      "Epoch 351 train_loss: 0.0000345734 val_loss: 0.1122656104\n",
      "Epoch 352 train_loss: 0.0000368753 val_loss: 0.1125893831\n",
      "Epoch 353 train_loss: 0.0000557017 val_loss: 0.1128217926\n",
      "Epoch 354 train_loss: 0.0000237186 val_loss: 0.1123590442\n",
      "Epoch 355 train_loss: 0.0000462325 val_loss: 0.1129027115\n",
      "Epoch 356 train_loss: 0.0000620691 val_loss: 0.1131986544\n",
      "Epoch 357 train_loss: 0.0000484983 val_loss: 0.1119659218\n",
      "Epoch 358 train_loss: 0.0000305889 val_loss: 0.1133972114\n",
      "Epoch 359 train_loss: 0.0000474067 val_loss: 0.1118694285\n",
      "Epoch 360 train_loss: 0.0000472724 val_loss: 0.1130262000\n",
      "Epoch 361 train_loss: 0.0000313853 val_loss: 0.1128692978\n",
      "Epoch 362 train_loss: 0.0000258626 val_loss: 0.1135595949\n",
      "Epoch 363 train_loss: 0.0001180150 val_loss: 0.1128317776\n",
      "Epoch 364 train_loss: 0.0000203717 val_loss: 0.1129775731\n",
      "Epoch 365 train_loss: 0.0000537737 val_loss: 0.1132166999\n",
      "Epoch 366 train_loss: 0.0000768571 val_loss: 0.1133769087\n",
      "Epoch 367 train_loss: 0.0000463275 val_loss: 0.1131359565\n",
      "Epoch 368 train_loss: 0.0000254095 val_loss: 0.1131424087\n",
      "Epoch 369 train_loss: 0.0000624376 val_loss: 0.1129702182\n",
      "Epoch 370 train_loss: 0.0000339960 val_loss: 0.1131450073\n",
      "Epoch 371 train_loss: 0.0000325288 val_loss: 0.1132589370\n",
      "Epoch 372 train_loss: 0.0000423937 val_loss: 0.1130872250\n",
      "Epoch 373 train_loss: 0.0000527547 val_loss: 0.1129249920\n",
      "Epoch 374 train_loss: 0.0000724457 val_loss: 0.1129389245\n",
      "Epoch 375 train_loss: 0.0000638078 val_loss: 0.1128145689\n",
      "Epoch 376 train_loss: 0.0000767321 val_loss: 0.1127888659\n",
      "Epoch 377 train_loss: 0.0000556424 val_loss: 0.1129781052\n",
      "Epoch 378 train_loss: 0.0000366351 val_loss: 0.1127364447\n",
      "Epoch 379 train_loss: 0.0000418834 val_loss: 0.1133867906\n",
      "Epoch 380 train_loss: 0.0000226388 val_loss: 0.1127859676\n",
      "Epoch 381 train_loss: 0.0000632344 val_loss: 0.1133613519\n",
      "Epoch 382 train_loss: 0.0000384076 val_loss: 0.1135688467\n",
      "Epoch 383 train_loss: 0.0000390363 val_loss: 0.1130458845\n",
      "Epoch 384 train_loss: 0.0000497932 val_loss: 0.1129642140\n",
      "Epoch 385 train_loss: 0.0001655132 val_loss: 0.1128788779\n",
      "Epoch 386 train_loss: 0.0000574832 val_loss: 0.1131240442\n",
      "Epoch 387 train_loss: 0.0000615828 val_loss: 0.1121406337\n",
      "Epoch 388 train_loss: 0.0000291011 val_loss: 0.1133769075\n",
      "Epoch 389 train_loss: 0.0000347346 val_loss: 0.1141349616\n",
      "Epoch 390 train_loss: 0.0000219909 val_loss: 0.1134476840\n",
      "Epoch 391 train_loss: 0.0000178848 val_loss: 0.1135048465\n",
      "Epoch 392 train_loss: 0.0000159908 val_loss: 0.1124583682\n",
      "Epoch 393 train_loss: 0.0000142029 val_loss: 0.1131015641\n",
      "Epoch 394 train_loss: 0.0000525704 val_loss: 0.1142785622\n",
      "Epoch 395 train_loss: 0.0000288556 val_loss: 0.1130369310\n",
      "Epoch 396 train_loss: 0.0000309652 val_loss: 0.1136454271\n",
      "Epoch 397 train_loss: 0.0000419857 val_loss: 0.1122257830\n",
      "Epoch 398 train_loss: 0.0000218086 val_loss: 0.1129259089\n",
      "Epoch 399 train_loss: 0.0000307740 val_loss: 0.1120005288\n",
      "Epoch 400 train_loss: 0.0000450135 val_loss: 0.1133478274\n",
      "Epoch 401 train_loss: 0.0000524981 val_loss: 0.1129402609\n",
      "Epoch 402 train_loss: 0.0000322371 val_loss: 0.1124177545\n",
      "Epoch 403 train_loss: 0.0000806132 val_loss: 0.1134973805\n",
      "Epoch 404 train_loss: 0.0000432266 val_loss: 0.1132338495\n",
      "Epoch 405 train_loss: 0.0000487201 val_loss: 0.1127711018\n",
      "Epoch 406 train_loss: 0.0000255688 val_loss: 0.1130219243\n",
      "Epoch 407 train_loss: 0.0000480332 val_loss: 0.1130614302\n",
      "Epoch 408 train_loss: 0.0000122587 val_loss: 0.1130813664\n",
      "Epoch 409 train_loss: 0.0000414716 val_loss: 0.1136541399\n",
      "Epoch 410 train_loss: 0.0000276933 val_loss: 0.1125078793\n",
      "Epoch 411 train_loss: 0.0000450231 val_loss: 0.1128813805\n",
      "Epoch 412 train_loss: 0.0000240763 val_loss: 0.1132008687\n",
      "Epoch 413 train_loss: 0.0001264758 val_loss: 0.1130023249\n",
      "Epoch 414 train_loss: 0.0001356503 val_loss: 0.1132860338\n",
      "Epoch 415 train_loss: 0.0000221157 val_loss: 0.1131018490\n",
      "Epoch 416 train_loss: 0.0000304699 val_loss: 0.1127435037\n",
      "Epoch 417 train_loss: 0.0000372990 val_loss: 0.1135131616\n",
      "Epoch 418 train_loss: 0.0000822052 val_loss: 0.1133767973\n",
      "Epoch 419 train_loss: 0.0000374830 val_loss: 0.1128314842\n",
      "Epoch 420 train_loss: 0.0000315185 val_loss: 0.1133924083\n",
      "Epoch 421 train_loss: 0.0000110149 val_loss: 0.1134521579\n",
      "Epoch 422 train_loss: 0.0000681976 val_loss: 0.1122317146\n",
      "Epoch 423 train_loss: 0.0000197061 val_loss: 0.1136616580\n",
      "Epoch 424 train_loss: 0.0000241264 val_loss: 0.1125788679\n",
      "Epoch 425 train_loss: 0.0000226283 val_loss: 0.1135195819\n",
      "Epoch 426 train_loss: 0.0000971246 val_loss: 0.1133992982\n",
      "Epoch 427 train_loss: 0.0000427266 val_loss: 0.1133228549\n",
      "Epoch 428 train_loss: 0.0000646316 val_loss: 0.1123800030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429 train_loss: 0.0000118656 val_loss: 0.1123911489\n",
      "Epoch 430 train_loss: 0.0000178100 val_loss: 0.1131179688\n",
      "Epoch 431 train_loss: 0.0000374779 val_loss: 0.1129065006\n",
      "Epoch 432 train_loss: 0.0000408917 val_loss: 0.1129593590\n",
      "Epoch 433 train_loss: 0.0000116737 val_loss: 0.1132803836\n",
      "Epoch 434 train_loss: 0.0000138911 val_loss: 0.1131741433\n",
      "Epoch 435 train_loss: 0.0000208323 val_loss: 0.1130250741\n",
      "Epoch 436 train_loss: 0.0000752475 val_loss: 0.1126197520\n",
      "Epoch 437 train_loss: 0.0000496523 val_loss: 0.1133167752\n",
      "Epoch 438 train_loss: 0.0000307733 val_loss: 0.1130843276\n",
      "Epoch 439 train_loss: 0.0000463752 val_loss: 0.1129454538\n",
      "Epoch 440 train_loss: 0.0000312344 val_loss: 0.1130785373\n",
      "Epoch 441 train_loss: 0.0000329663 val_loss: 0.1135615173\n",
      "Epoch 442 train_loss: 0.0000453808 val_loss: 0.1135539762\n",
      "Epoch 443 train_loss: 0.0000673871 val_loss: 0.1127085268\n",
      "Epoch 444 train_loss: 0.0000286970 val_loss: 0.1134091808\n",
      "Epoch 445 train_loss: 0.0000588041 val_loss: 0.1133203986\n",
      "Epoch 446 train_loss: 0.0000366483 val_loss: 0.1131862025\n",
      "Epoch 447 train_loss: 0.0000463767 val_loss: 0.1127089450\n",
      "Epoch 448 train_loss: 0.0000272028 val_loss: 0.1121978620\n",
      "Epoch 449 train_loss: 0.0000644050 val_loss: 0.1133901022\n",
      "Epoch 450 train_loss: 0.0000157716 val_loss: 0.1126757005\n",
      "Epoch 451 train_loss: 0.0000776616 val_loss: 0.1126478679\n",
      "Epoch 452 train_loss: 0.0000225197 val_loss: 0.1132050600\n",
      "Epoch 453 train_loss: 0.0000424598 val_loss: 0.1131444036\n",
      "Epoch 454 train_loss: 0.0000354259 val_loss: 0.1130983487\n",
      "Epoch 455 train_loss: 0.0001348178 val_loss: 0.1126139740\n",
      "Epoch 456 train_loss: 0.0000167430 val_loss: 0.1130541394\n",
      "Epoch 457 train_loss: 0.0000454095 val_loss: 0.1130208114\n",
      "Epoch 458 train_loss: 0.0000361144 val_loss: 0.1131784282\n",
      "Epoch 459 train_loss: 0.0000747372 val_loss: 0.1124581458\n",
      "Epoch 460 train_loss: 0.0000238693 val_loss: 0.1137664307\n",
      "Epoch 461 train_loss: 0.0000360217 val_loss: 0.1129526570\n",
      "Epoch 462 train_loss: 0.0000382763 val_loss: 0.1129084643\n",
      "Epoch 463 train_loss: 0.0000252255 val_loss: 0.1136026919\n",
      "Epoch 464 train_loss: 0.0000633360 val_loss: 0.1132148781\n",
      "Epoch 465 train_loss: 0.0000369756 val_loss: 0.1127200961\n",
      "Epoch 466 train_loss: 0.0000264041 val_loss: 0.1133435455\n",
      "Epoch 467 train_loss: 0.0000715931 val_loss: 0.1128271669\n",
      "Epoch 468 train_loss: 0.0000201504 val_loss: 0.1129852161\n",
      "Epoch 469 train_loss: 0.0000499638 val_loss: 0.1123700623\n",
      "Epoch 470 train_loss: 0.0000205951 val_loss: 0.1127967791\n",
      "Epoch 471 train_loss: 0.0001015865 val_loss: 0.1129093589\n",
      "Epoch 472 train_loss: 0.0000235071 val_loss: 0.1134432936\n",
      "Epoch 473 train_loss: 0.0000557161 val_loss: 0.1126495770\n",
      "Epoch 474 train_loss: 0.0000343738 val_loss: 0.1135720479\n",
      "Epoch 475 train_loss: 0.0000200640 val_loss: 0.1135487531\n",
      "Epoch 476 train_loss: 0.0000796980 val_loss: 0.1126067950\n",
      "Epoch 477 train_loss: 0.0000292095 val_loss: 0.1129742784\n",
      "Epoch 478 train_loss: 0.0000205082 val_loss: 0.1132163549\n",
      "Epoch 479 train_loss: 0.0000243576 val_loss: 0.1131532520\n",
      "Epoch 480 train_loss: 0.0000549983 val_loss: 0.1136805389\n",
      "Epoch 481 train_loss: 0.0000887672 val_loss: 0.1126759032\n",
      "Epoch 482 train_loss: 0.0000217591 val_loss: 0.1143732897\n",
      "Epoch 483 train_loss: 0.0000546706 val_loss: 0.1137388373\n",
      "Epoch 484 train_loss: 0.0000354478 val_loss: 0.1131312657\n",
      "Epoch 485 train_loss: 0.0000520631 val_loss: 0.1126691732\n",
      "Epoch 486 train_loss: 0.0000228736 val_loss: 0.1132767123\n",
      "Epoch 487 train_loss: 0.0001133838 val_loss: 0.1131690084\n",
      "Epoch 488 train_loss: 0.0000087620 val_loss: 0.1129636206\n",
      "Epoch 489 train_loss: 0.0000214704 val_loss: 0.1128963082\n",
      "Epoch 490 train_loss: 0.0000513323 val_loss: 0.1130423484\n",
      "Epoch 491 train_loss: 0.0000331237 val_loss: 0.1130044975\n",
      "Epoch 492 train_loss: 0.0000379226 val_loss: 0.1137173211\n",
      "Epoch 493 train_loss: 0.0000330328 val_loss: 0.1134993106\n",
      "Epoch 494 train_loss: 0.0000276800 val_loss: 0.1132030556\n",
      "Epoch 495 train_loss: 0.0000521159 val_loss: 0.1132060642\n",
      "Epoch 496 train_loss: 0.0000112941 val_loss: 0.1134103183\n",
      "Epoch 497 train_loss: 0.0000248731 val_loss: 0.1134589010\n",
      "Epoch 498 train_loss: 0.0000959121 val_loss: 0.1133088410\n",
      "Epoch 499 train_loss: 0.0000567171 val_loss: 0.1131068441\n",
      "Epoch 500 train_loss: 0.0000462124 val_loss: 0.1139239753\n"
     ]
    }
   ],
   "source": [
    "# training for standard LSTM\n",
    "\n",
    "epochs = 500  # This is low, typically you'd have more epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "        single_loss = criterion(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            y_pred = model(inputs)\n",
    "            val_loss += criterion(y_pred, labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1} train_loss: {single_loss.item():.10f} val_loss: {val_loss:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7ae88c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.0000303562 val_loss: 0.1128592911\n",
      "Epoch 2 train_loss: 0.0000213959 val_loss: 0.1127702783\n",
      "Epoch 3 train_loss: 0.0000640498 val_loss: 0.1129590200\n",
      "Epoch 4 train_loss: 0.0000185229 val_loss: 0.1135864391\n",
      "Epoch 5 train_loss: 0.0000662308 val_loss: 0.1128986334\n",
      "Epoch 6 train_loss: 0.0000256227 val_loss: 0.1134182930\n",
      "Epoch 7 train_loss: 0.0000200374 val_loss: 0.1134850560\n",
      "Epoch 8 train_loss: 0.0000191856 val_loss: 0.1144921553\n",
      "Epoch 9 train_loss: 0.0000222398 val_loss: 0.1133078688\n",
      "Epoch 10 train_loss: 0.0000702690 val_loss: 0.1132656295\n",
      "Epoch 11 train_loss: 0.0000598497 val_loss: 0.1136993747\n",
      "Epoch 12 train_loss: 0.0000453049 val_loss: 0.1139200297\n",
      "Epoch 13 train_loss: 0.0000293631 val_loss: 0.1134562693\n",
      "Epoch 14 train_loss: 0.0000174231 val_loss: 0.1127772885\n",
      "Epoch 15 train_loss: 0.0000870790 val_loss: 0.1135870725\n",
      "Epoch 16 train_loss: 0.0000271910 val_loss: 0.1124069034\n",
      "Epoch 17 train_loss: 0.0000288827 val_loss: 0.1135820561\n",
      "Epoch 18 train_loss: 0.0000618623 val_loss: 0.1132887198\n",
      "Epoch 19 train_loss: 0.0000298095 val_loss: 0.1123953307\n",
      "Epoch 20 train_loss: 0.0000286098 val_loss: 0.1133739985\n",
      "Epoch 21 train_loss: 0.0000305615 val_loss: 0.1127383992\n",
      "Epoch 22 train_loss: 0.0000217025 val_loss: 0.1132637421\n",
      "Epoch 23 train_loss: 0.0000232459 val_loss: 0.1132983624\n",
      "Epoch 24 train_loss: 0.0000532959 val_loss: 0.1132847488\n",
      "Epoch 25 train_loss: 0.0000222576 val_loss: 0.1133788090\n",
      "Epoch 26 train_loss: 0.0000546280 val_loss: 0.1127204722\n",
      "Epoch 27 train_loss: 0.0000510643 val_loss: 0.1130180007\n",
      "Epoch 28 train_loss: 0.0000428012 val_loss: 0.1130320130\n",
      "Epoch 29 train_loss: 0.0000397105 val_loss: 0.1130154195\n",
      "Epoch 30 train_loss: 0.0000183480 val_loss: 0.1129872390\n",
      "Epoch 31 train_loss: 0.0000688490 val_loss: 0.1132079540\n",
      "Epoch 32 train_loss: 0.0000149438 val_loss: 0.1127564167\n",
      "Epoch 33 train_loss: 0.0000797798 val_loss: 0.1127889070\n",
      "Epoch 34 train_loss: 0.0000305406 val_loss: 0.1128883883\n",
      "Epoch 35 train_loss: 0.0000117724 val_loss: 0.1130771102\n",
      "Epoch 36 train_loss: 0.0000207054 val_loss: 0.1127144472\n",
      "Epoch 37 train_loss: 0.0000200714 val_loss: 0.1133181861\n",
      "Epoch 38 train_loss: 0.0000178638 val_loss: 0.1136022039\n",
      "Epoch 39 train_loss: 0.0000434520 val_loss: 0.1121611382\n",
      "Epoch 40 train_loss: 0.0000393059 val_loss: 0.1124882476\n",
      "Epoch 41 train_loss: 0.0000814654 val_loss: 0.1128997876\n",
      "Epoch 42 train_loss: 0.0000215495 val_loss: 0.1134539187\n",
      "Epoch 43 train_loss: 0.0000294081 val_loss: 0.1134745163\n",
      "Epoch 44 train_loss: 0.0000056034 val_loss: 0.1137090435\n",
      "Epoch 45 train_loss: 0.0000605270 val_loss: 0.1134382886\n",
      "Epoch 46 train_loss: 0.0000265661 val_loss: 0.1129374751\n",
      "Epoch 47 train_loss: 0.0000732686 val_loss: 0.1132899760\n",
      "Epoch 48 train_loss: 0.0000412733 val_loss: 0.1127562517\n",
      "Epoch 49 train_loss: 0.0000160572 val_loss: 0.1135611936\n",
      "Epoch 50 train_loss: 0.0000392305 val_loss: 0.1130037585\n",
      "Epoch 51 train_loss: 0.0000877917 val_loss: 0.1126354415\n",
      "Epoch 52 train_loss: 0.0000105358 val_loss: 0.1128978598\n",
      "Epoch 53 train_loss: 0.0000484618 val_loss: 0.1124777490\n",
      "Epoch 54 train_loss: 0.0000471959 val_loss: 0.1131356898\n",
      "Epoch 55 train_loss: 0.0000196385 val_loss: 0.1131157367\n",
      "Epoch 56 train_loss: 0.0000332530 val_loss: 0.1137249332\n",
      "Epoch 57 train_loss: 0.0000079472 val_loss: 0.1135722752\n",
      "Epoch 58 train_loss: 0.0000349931 val_loss: 0.1131294436\n",
      "Epoch 59 train_loss: 0.0000209639 val_loss: 0.1126049664\n",
      "Epoch 60 train_loss: 0.0000278758 val_loss: 0.1136745301\n",
      "Epoch 61 train_loss: 0.0000506056 val_loss: 0.1124732752\n",
      "Epoch 62 train_loss: 0.0000889579 val_loss: 0.1124953172\n",
      "Epoch 63 train_loss: 0.0000286220 val_loss: 0.1138153831\n",
      "Epoch 64 train_loss: 0.0000100141 val_loss: 0.1130100094\n",
      "Epoch 65 train_loss: 0.0000523771 val_loss: 0.1138340801\n",
      "Epoch 66 train_loss: 0.0000251160 val_loss: 0.1128353085\n",
      "Epoch 67 train_loss: 0.0000103711 val_loss: 0.1127236014\n",
      "Epoch 68 train_loss: 0.0000908322 val_loss: 0.1128430736\n",
      "Epoch 69 train_loss: 0.0000156729 val_loss: 0.1132984253\n",
      "Epoch 70 train_loss: 0.0000117579 val_loss: 0.1128275759\n",
      "Epoch 71 train_loss: 0.0000470874 val_loss: 0.1131673019\n",
      "Epoch 72 train_loss: 0.0000057048 val_loss: 0.1134401032\n",
      "Epoch 73 train_loss: 0.0000308561 val_loss: 0.1132586776\n",
      "Epoch 74 train_loss: 0.0000429317 val_loss: 0.1126646996\n",
      "Epoch 75 train_loss: 0.0000622367 val_loss: 0.1129920561\n",
      "Epoch 76 train_loss: 0.0000177899 val_loss: 0.1129457256\n",
      "Epoch 77 train_loss: 0.0000258131 val_loss: 0.1127135584\n",
      "Epoch 78 train_loss: 0.0000383814 val_loss: 0.1132666981\n",
      "Epoch 79 train_loss: 0.0000427071 val_loss: 0.1135047250\n",
      "Epoch 80 train_loss: 0.0000330548 val_loss: 0.1125160624\n",
      "Epoch 81 train_loss: 0.0000168206 val_loss: 0.1133942845\n",
      "Epoch 82 train_loss: 0.0001013589 val_loss: 0.1127052976\n",
      "Epoch 83 train_loss: 0.0000807526 val_loss: 0.1127863992\n",
      "Epoch 84 train_loss: 0.0000278994 val_loss: 0.1120276610\n",
      "Epoch 85 train_loss: 0.0000174133 val_loss: 0.1129767365\n",
      "Epoch 86 train_loss: 0.0000347134 val_loss: 0.1125904165\n",
      "Epoch 87 train_loss: 0.0000704589 val_loss: 0.1129841968\n",
      "Epoch 88 train_loss: 0.0000208048 val_loss: 0.1135388951\n",
      "Epoch 89 train_loss: 0.0000094146 val_loss: 0.1127690261\n",
      "Epoch 90 train_loss: 0.0000159730 val_loss: 0.1130438206\n",
      "Epoch 91 train_loss: 0.0000535617 val_loss: 0.1128523210\n",
      "Epoch 92 train_loss: 0.0000317301 val_loss: 0.1129168857\n",
      "Epoch 93 train_loss: 0.0000239530 val_loss: 0.1125711280\n",
      "Epoch 94 train_loss: 0.0000301417 val_loss: 0.1127544311\n",
      "Epoch 95 train_loss: 0.0000434628 val_loss: 0.1129682027\n",
      "Epoch 96 train_loss: 0.0000292376 val_loss: 0.1126258545\n",
      "Epoch 97 train_loss: 0.0000482950 val_loss: 0.1135586793\n",
      "Epoch 98 train_loss: 0.0000227830 val_loss: 0.1134195712\n",
      "Epoch 99 train_loss: 0.0000333828 val_loss: 0.1134159247\n",
      "Epoch 100 train_loss: 0.0000704277 val_loss: 0.1128277604\n",
      "Epoch 101 train_loss: 0.0000153253 val_loss: 0.1131583836\n",
      "Epoch 102 train_loss: 0.0000167443 val_loss: 0.1128024491\n",
      "Epoch 103 train_loss: 0.0000306399 val_loss: 0.1131371043\n",
      "Epoch 104 train_loss: 0.0000821745 val_loss: 0.1130308707\n",
      "Epoch 105 train_loss: 0.0000504887 val_loss: 0.1134736478\n",
      "Epoch 106 train_loss: 0.0000536235 val_loss: 0.1133148785\n",
      "Epoch 107 train_loss: 0.0000345485 val_loss: 0.1134451336\n",
      "Epoch 108 train_loss: 0.0000479149 val_loss: 0.1130152120\n",
      "Epoch 109 train_loss: 0.0000301314 val_loss: 0.1135994629\n",
      "Epoch 110 train_loss: 0.0000378828 val_loss: 0.1137959113\n",
      "Epoch 111 train_loss: 0.0000929086 val_loss: 0.1129747129\n",
      "Epoch 112 train_loss: 0.0000462171 val_loss: 0.1127714187\n",
      "Epoch 113 train_loss: 0.0000332883 val_loss: 0.1136585597\n",
      "Epoch 114 train_loss: 0.0000292958 val_loss: 0.1129209988\n",
      "Epoch 115 train_loss: 0.0000122065 val_loss: 0.1132772431\n",
      "Epoch 116 train_loss: 0.0000416352 val_loss: 0.1140747758\n",
      "Epoch 117 train_loss: 0.0000072362 val_loss: 0.1128237445\n",
      "Epoch 118 train_loss: 0.0000130522 val_loss: 0.1132723383\n",
      "Epoch 119 train_loss: 0.0000415390 val_loss: 0.1126441259\n",
      "Epoch 120 train_loss: 0.0000420557 val_loss: 0.1127994399\n",
      "Epoch 121 train_loss: 0.0000409504 val_loss: 0.1135221779\n",
      "Epoch 122 train_loss: 0.0000152036 val_loss: 0.1123455409\n",
      "Epoch 123 train_loss: 0.0000505901 val_loss: 0.1126099153\n",
      "Epoch 124 train_loss: 0.0000652320 val_loss: 0.1131396475\n",
      "Epoch 125 train_loss: 0.0000494409 val_loss: 0.1129028258\n",
      "Epoch 126 train_loss: 0.0000322885 val_loss: 0.1134325437\n",
      "Epoch 127 train_loss: 0.0000196208 val_loss: 0.1129130584\n",
      "Epoch 128 train_loss: 0.0000109442 val_loss: 0.1131132490\n",
      "Epoch 129 train_loss: 0.0000201235 val_loss: 0.1126965767\n",
      "Epoch 130 train_loss: 0.0000216149 val_loss: 0.1133419680\n",
      "Epoch 131 train_loss: 0.0000303874 val_loss: 0.1134053706\n",
      "Epoch 132 train_loss: 0.0000298919 val_loss: 0.1127949335\n",
      "Epoch 133 train_loss: 0.0000226029 val_loss: 0.1128956453\n",
      "Epoch 134 train_loss: 0.0000489229 val_loss: 0.1129738893\n",
      "Epoch 135 train_loss: 0.0000691927 val_loss: 0.1126344463\n",
      "Epoch 136 train_loss: 0.0001057142 val_loss: 0.1134840199\n",
      "Epoch 137 train_loss: 0.0000286625 val_loss: 0.1125286731\n",
      "Epoch 138 train_loss: 0.0000302891 val_loss: 0.1130461960\n",
      "Epoch 139 train_loss: 0.0000280404 val_loss: 0.1127738805\n",
      "Epoch 140 train_loss: 0.0000612132 val_loss: 0.1135849495\n",
      "Epoch 141 train_loss: 0.0000547019 val_loss: 0.1129392124\n",
      "Epoch 142 train_loss: 0.0000190259 val_loss: 0.1131745967\n",
      "Epoch 143 train_loss: 0.0000344910 val_loss: 0.1127440367\n",
      "Epoch 144 train_loss: 0.0000267616 val_loss: 0.1138422573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 train_loss: 0.0000486862 val_loss: 0.1126988713\n",
      "Epoch 146 train_loss: 0.0000279840 val_loss: 0.1136871686\n",
      "Epoch 147 train_loss: 0.0000368897 val_loss: 0.1135617509\n",
      "Epoch 148 train_loss: 0.0000382988 val_loss: 0.1133388689\n",
      "Epoch 149 train_loss: 0.0000852965 val_loss: 0.1132500482\n",
      "Epoch 150 train_loss: 0.0000383389 val_loss: 0.1129043463\n",
      "Epoch 151 train_loss: 0.0000291873 val_loss: 0.1129246515\n",
      "Epoch 152 train_loss: 0.0000812634 val_loss: 0.1130787512\n",
      "Epoch 153 train_loss: 0.0000284790 val_loss: 0.1137065623\n",
      "Epoch 154 train_loss: 0.0000845811 val_loss: 0.1130117131\n",
      "Epoch 155 train_loss: 0.0000786131 val_loss: 0.1134896274\n",
      "Epoch 156 train_loss: 0.0000162275 val_loss: 0.1128578204\n",
      "Epoch 157 train_loss: 0.0000192513 val_loss: 0.1132501468\n",
      "Epoch 158 train_loss: 0.0000941601 val_loss: 0.1131697019\n",
      "Epoch 159 train_loss: 0.0000415193 val_loss: 0.1130925815\n",
      "Epoch 160 train_loss: 0.0000249289 val_loss: 0.1123735031\n",
      "Epoch 161 train_loss: 0.0000162950 val_loss: 0.1130256081\n",
      "Epoch 162 train_loss: 0.0000231846 val_loss: 0.1130372500\n",
      "Epoch 163 train_loss: 0.0000490313 val_loss: 0.1132234672\n",
      "Epoch 164 train_loss: 0.0000424547 val_loss: 0.1134229304\n",
      "Epoch 165 train_loss: 0.0000107831 val_loss: 0.1131332262\n",
      "Epoch 166 train_loss: 0.0000509232 val_loss: 0.1135088586\n",
      "Epoch 167 train_loss: 0.0000134366 val_loss: 0.1129824840\n",
      "Epoch 168 train_loss: 0.0000170181 val_loss: 0.1132943289\n",
      "Epoch 169 train_loss: 0.0000275621 val_loss: 0.1122544868\n",
      "Epoch 170 train_loss: 0.0000281937 val_loss: 0.1126053858\n",
      "Epoch 171 train_loss: 0.0000424081 val_loss: 0.1124862932\n",
      "Epoch 172 train_loss: 0.0000220618 val_loss: 0.1135388267\n",
      "Epoch 173 train_loss: 0.0000611949 val_loss: 0.1128912849\n",
      "Epoch 174 train_loss: 0.0000257200 val_loss: 0.1131695252\n",
      "Epoch 175 train_loss: 0.0000285527 val_loss: 0.1133808884\n",
      "Epoch 176 train_loss: 0.0000160115 val_loss: 0.1130430499\n",
      "Epoch 177 train_loss: 0.0000299648 val_loss: 0.1131252562\n",
      "Epoch 178 train_loss: 0.0000332372 val_loss: 0.1129425264\n",
      "Epoch 179 train_loss: 0.0000222052 val_loss: 0.1139372807\n",
      "Epoch 180 train_loss: 0.0000747899 val_loss: 0.1132562444\n",
      "Epoch 181 train_loss: 0.0000563069 val_loss: 0.1133048698\n",
      "Epoch 182 train_loss: 0.0000828583 val_loss: 0.1131881251\n",
      "Epoch 183 train_loss: 0.0000743435 val_loss: 0.1128143640\n",
      "Epoch 184 train_loss: 0.0000122420 val_loss: 0.1130934853\n",
      "Epoch 185 train_loss: 0.0000525742 val_loss: 0.1128241187\n",
      "Epoch 186 train_loss: 0.0000251378 val_loss: 0.1133006685\n",
      "Epoch 187 train_loss: 0.0000222275 val_loss: 0.1127935542\n",
      "Epoch 188 train_loss: 0.0001279077 val_loss: 0.1123770935\n",
      "Epoch 189 train_loss: 0.0001158908 val_loss: 0.1136575349\n",
      "Epoch 190 train_loss: 0.0000322794 val_loss: 0.1122250314\n",
      "Epoch 191 train_loss: 0.0000283143 val_loss: 0.1130276070\n",
      "Epoch 192 train_loss: 0.0000149921 val_loss: 0.1131877342\n",
      "Epoch 193 train_loss: 0.0000463156 val_loss: 0.1133631816\n",
      "Epoch 194 train_loss: 0.0000171023 val_loss: 0.1129561241\n",
      "Epoch 195 train_loss: 0.0000434459 val_loss: 0.1136004430\n",
      "Epoch 196 train_loss: 0.0000506577 val_loss: 0.1131384262\n",
      "Epoch 197 train_loss: 0.0000197423 val_loss: 0.1123480306\n",
      "Epoch 198 train_loss: 0.0000376518 val_loss: 0.1137820465\n",
      "Epoch 199 train_loss: 0.0000856808 val_loss: 0.1130766061\n",
      "Epoch 200 train_loss: 0.0000182553 val_loss: 0.1127300380\n",
      "Epoch 201 train_loss: 0.0000178412 val_loss: 0.1135761054\n",
      "Epoch 202 train_loss: 0.0000672280 val_loss: 0.1122837514\n",
      "Epoch 203 train_loss: 0.0000682517 val_loss: 0.1136545474\n",
      "Epoch 204 train_loss: 0.0000126003 val_loss: 0.1132957274\n",
      "Epoch 205 train_loss: 0.0000390389 val_loss: 0.1136002822\n",
      "Epoch 206 train_loss: 0.0000612895 val_loss: 0.1132128334\n",
      "Epoch 207 train_loss: 0.0000432317 val_loss: 0.1131020757\n",
      "Epoch 208 train_loss: 0.0000788720 val_loss: 0.1123382212\n",
      "Epoch 209 train_loss: 0.0000166151 val_loss: 0.1132288067\n",
      "Epoch 210 train_loss: 0.0000285053 val_loss: 0.1131000081\n",
      "Epoch 211 train_loss: 0.0000297500 val_loss: 0.1126798674\n",
      "Epoch 212 train_loss: 0.0000934370 val_loss: 0.1129346262\n",
      "Epoch 213 train_loss: 0.0000490649 val_loss: 0.1131108215\n",
      "Epoch 214 train_loss: 0.0000284454 val_loss: 0.1132421786\n",
      "Epoch 215 train_loss: 0.0000137350 val_loss: 0.1131499815\n",
      "Epoch 216 train_loss: 0.0000378223 val_loss: 0.1135127214\n",
      "Epoch 217 train_loss: 0.0000181986 val_loss: 0.1129130587\n",
      "Epoch 218 train_loss: 0.0000619993 val_loss: 0.1125973557\n",
      "Epoch 219 train_loss: 0.0000597409 val_loss: 0.1135875885\n",
      "Epoch 220 train_loss: 0.0000459957 val_loss: 0.1127065787\n",
      "Epoch 221 train_loss: 0.0000523397 val_loss: 0.1125063383\n",
      "Epoch 222 train_loss: 0.0000683431 val_loss: 0.1131330700\n",
      "Epoch 223 train_loss: 0.0000263521 val_loss: 0.1129826244\n",
      "Epoch 224 train_loss: 0.0000195977 val_loss: 0.1125495067\n",
      "Epoch 225 train_loss: 0.0000509550 val_loss: 0.1128132368\n",
      "Epoch 226 train_loss: 0.0000296685 val_loss: 0.1129178995\n",
      "Epoch 227 train_loss: 0.0000351112 val_loss: 0.1131273102\n",
      "Epoch 228 train_loss: 0.0000470566 val_loss: 0.1131109660\n",
      "Epoch 229 train_loss: 0.0000210321 val_loss: 0.1133858832\n",
      "Epoch 230 train_loss: 0.0000933806 val_loss: 0.1126174901\n",
      "Epoch 231 train_loss: 0.0000107432 val_loss: 0.1123455751\n",
      "Epoch 232 train_loss: 0.0000311487 val_loss: 0.1130963110\n",
      "Epoch 233 train_loss: 0.0000118163 val_loss: 0.1127500247\n",
      "Epoch 234 train_loss: 0.0000399736 val_loss: 0.1132919818\n",
      "Epoch 235 train_loss: 0.0000854825 val_loss: 0.1133913629\n",
      "Epoch 236 train_loss: 0.0000097102 val_loss: 0.1134605256\n",
      "Epoch 237 train_loss: 0.0000501917 val_loss: 0.1130063843\n",
      "Epoch 238 train_loss: 0.0000280469 val_loss: 0.1127942765\n",
      "Epoch 239 train_loss: 0.0000504063 val_loss: 0.1134781289\n",
      "Epoch 240 train_loss: 0.0000191105 val_loss: 0.1135296632\n",
      "Epoch 241 train_loss: 0.0000169031 val_loss: 0.1130824947\n",
      "Epoch 242 train_loss: 0.0000285038 val_loss: 0.1130847087\n",
      "Epoch 243 train_loss: 0.0000468071 val_loss: 0.1133354690\n",
      "Epoch 244 train_loss: 0.0000322628 val_loss: 0.1132310839\n",
      "Epoch 245 train_loss: 0.0000209497 val_loss: 0.1127163811\n",
      "Epoch 246 train_loss: 0.0000264369 val_loss: 0.1126259919\n",
      "Epoch 247 train_loss: 0.0000645697 val_loss: 0.1120339326\n",
      "Epoch 248 train_loss: 0.0000410421 val_loss: 0.1133954543\n",
      "Epoch 249 train_loss: 0.0000553591 val_loss: 0.1135372511\n",
      "Epoch 250 train_loss: 0.0000966774 val_loss: 0.1129736069\n",
      "Epoch 251 train_loss: 0.0000564867 val_loss: 0.1135335726\n",
      "Epoch 252 train_loss: 0.0000290825 val_loss: 0.1131058583\n",
      "Epoch 253 train_loss: 0.0000250136 val_loss: 0.1135491952\n",
      "Epoch 254 train_loss: 0.0000207368 val_loss: 0.1123231740\n",
      "Epoch 255 train_loss: 0.0000607530 val_loss: 0.1123998421\n",
      "Epoch 256 train_loss: 0.0000384167 val_loss: 0.1133065996\n",
      "Epoch 257 train_loss: 0.0000198444 val_loss: 0.1133072913\n",
      "Epoch 258 train_loss: 0.0000175649 val_loss: 0.1137939266\n",
      "Epoch 259 train_loss: 0.0000633711 val_loss: 0.1129030805\n",
      "Epoch 260 train_loss: 0.0000339579 val_loss: 0.1131144942\n",
      "Epoch 261 train_loss: 0.0000435157 val_loss: 0.1127303839\n",
      "Epoch 262 train_loss: 0.0000344271 val_loss: 0.1127725151\n",
      "Epoch 263 train_loss: 0.0000177529 val_loss: 0.1137600618\n",
      "Epoch 264 train_loss: 0.0000708145 val_loss: 0.1126748758\n",
      "Epoch 265 train_loss: 0.0000425375 val_loss: 0.1128954444\n",
      "Epoch 266 train_loss: 0.0000224268 val_loss: 0.1132295515\n",
      "Epoch 267 train_loss: 0.0000378788 val_loss: 0.1118508245\n",
      "Epoch 268 train_loss: 0.0001037489 val_loss: 0.1130030269\n",
      "Epoch 269 train_loss: 0.0000471703 val_loss: 0.1130044116\n",
      "Epoch 270 train_loss: 0.0000445797 val_loss: 0.1126704964\n",
      "Epoch 271 train_loss: 0.0000148103 val_loss: 0.1128038515\n",
      "Epoch 272 train_loss: 0.0000407599 val_loss: 0.1126589705\n",
      "Epoch 273 train_loss: 0.0001157745 val_loss: 0.1136656906\n",
      "Epoch 274 train_loss: 0.0000644986 val_loss: 0.1128665669\n",
      "Epoch 275 train_loss: 0.0000230951 val_loss: 0.1132397224\n",
      "Epoch 276 train_loss: 0.0000545576 val_loss: 0.1129794510\n",
      "Epoch 277 train_loss: 0.0000187312 val_loss: 0.1138283995\n",
      "Epoch 278 train_loss: 0.0000357040 val_loss: 0.1116601086\n",
      "Epoch 279 train_loss: 0.0000819758 val_loss: 0.1129268210\n",
      "Epoch 280 train_loss: 0.0000220742 val_loss: 0.1131905334\n",
      "Epoch 281 train_loss: 0.0000689005 val_loss: 0.1129757757\n",
      "Epoch 282 train_loss: 0.0000336210 val_loss: 0.1136210466\n",
      "Epoch 283 train_loss: 0.0000667138 val_loss: 0.1123356643\n",
      "Epoch 284 train_loss: 0.0000480405 val_loss: 0.1131115183\n",
      "Epoch 285 train_loss: 0.0000242170 val_loss: 0.1131507600\n",
      "Epoch 286 train_loss: 0.0000648658 val_loss: 0.1128561562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287 train_loss: 0.0000443852 val_loss: 0.1134607306\n",
      "Epoch 288 train_loss: 0.0000395676 val_loss: 0.1126946804\n",
      "Epoch 289 train_loss: 0.0000320514 val_loss: 0.1136373147\n",
      "Epoch 290 train_loss: 0.0000175913 val_loss: 0.1130173943\n",
      "Epoch 291 train_loss: 0.0000575636 val_loss: 0.1135580110\n",
      "Epoch 292 train_loss: 0.0000824633 val_loss: 0.1133471131\n",
      "Epoch 293 train_loss: 0.0000201892 val_loss: 0.1133541981\n",
      "Epoch 294 train_loss: 0.0000686329 val_loss: 0.1136529205\n",
      "Epoch 295 train_loss: 0.0000167777 val_loss: 0.1128197255\n",
      "Epoch 296 train_loss: 0.0000187607 val_loss: 0.1125279432\n",
      "Epoch 297 train_loss: 0.0000190627 val_loss: 0.1132606749\n",
      "Epoch 298 train_loss: 0.0000259911 val_loss: 0.1134939740\n",
      "Epoch 299 train_loss: 0.0000426764 val_loss: 0.1135088814\n",
      "Epoch 300 train_loss: 0.0000310807 val_loss: 0.1128562286\n",
      "Epoch 301 train_loss: 0.0000950317 val_loss: 0.1134852147\n",
      "Epoch 302 train_loss: 0.0000132017 val_loss: 0.1129333231\n",
      "Epoch 303 train_loss: 0.0000445164 val_loss: 0.1120873535\n",
      "Epoch 304 train_loss: 0.0000162284 val_loss: 0.1136497890\n",
      "Epoch 305 train_loss: 0.0000261829 val_loss: 0.1127612905\n",
      "Epoch 306 train_loss: 0.0000170667 val_loss: 0.1132195550\n",
      "Epoch 307 train_loss: 0.0000612281 val_loss: 0.1134384686\n",
      "Epoch 308 train_loss: 0.0000492983 val_loss: 0.1135501824\n",
      "Epoch 309 train_loss: 0.0000214051 val_loss: 0.1135073632\n",
      "Epoch 310 train_loss: 0.0000459602 val_loss: 0.1132422207\n",
      "Epoch 311 train_loss: 0.0000249178 val_loss: 0.1136966329\n",
      "Epoch 312 train_loss: 0.0000373560 val_loss: 0.1132120647\n",
      "Epoch 313 train_loss: 0.0000204552 val_loss: 0.1133660860\n",
      "Epoch 314 train_loss: 0.0000424204 val_loss: 0.1132767150\n",
      "Epoch 315 train_loss: 0.0000492105 val_loss: 0.1123268558\n",
      "Epoch 316 train_loss: 0.0000229625 val_loss: 0.1122509962\n",
      "Epoch 317 train_loss: 0.0000144799 val_loss: 0.1129355089\n",
      "Epoch 318 train_loss: 0.0001059580 val_loss: 0.1132310046\n",
      "Epoch 319 train_loss: 0.0000169538 val_loss: 0.1130370540\n",
      "Epoch 320 train_loss: 0.0000177403 val_loss: 0.1126401663\n",
      "Epoch 321 train_loss: 0.0000117368 val_loss: 0.1130250430\n",
      "Epoch 322 train_loss: 0.0000356233 val_loss: 0.1137200328\n",
      "Epoch 323 train_loss: 0.0000375811 val_loss: 0.1130558572\n",
      "Epoch 324 train_loss: 0.0000404628 val_loss: 0.1125421097\n",
      "Epoch 325 train_loss: 0.0000183019 val_loss: 0.1125406873\n",
      "Epoch 326 train_loss: 0.0000401977 val_loss: 0.1124384544\n",
      "Epoch 327 train_loss: 0.0000314423 val_loss: 0.1138781676\n",
      "Epoch 328 train_loss: 0.0000215726 val_loss: 0.1132751045\n",
      "Epoch 329 train_loss: 0.0000256304 val_loss: 0.1128349235\n",
      "Epoch 330 train_loss: 0.0000420791 val_loss: 0.1122996249\n",
      "Epoch 331 train_loss: 0.0000195358 val_loss: 0.1129119239\n",
      "Epoch 332 train_loss: 0.0000260469 val_loss: 0.1129231579\n",
      "Epoch 333 train_loss: 0.0000427855 val_loss: 0.1129543657\n",
      "Epoch 334 train_loss: 0.0000546358 val_loss: 0.1132623079\n",
      "Epoch 335 train_loss: 0.0000315709 val_loss: 0.1136084412\n",
      "Epoch 336 train_loss: 0.0000634070 val_loss: 0.1135249187\n",
      "Epoch 337 train_loss: 0.0000161060 val_loss: 0.1134178522\n",
      "Epoch 338 train_loss: 0.0000469890 val_loss: 0.1122007565\n",
      "Epoch 339 train_loss: 0.0000165231 val_loss: 0.1132916453\n",
      "Epoch 340 train_loss: 0.0000154508 val_loss: 0.1134966829\n",
      "Epoch 341 train_loss: 0.0000259031 val_loss: 0.1134757293\n",
      "Epoch 342 train_loss: 0.0000343579 val_loss: 0.1127210640\n",
      "Epoch 343 train_loss: 0.0001038106 val_loss: 0.1137777310\n",
      "Epoch 344 train_loss: 0.0000163868 val_loss: 0.1131791034\n",
      "Epoch 345 train_loss: 0.0000097686 val_loss: 0.1132136193\n",
      "Epoch 346 train_loss: 0.0000259709 val_loss: 0.1131299366\n",
      "Epoch 347 train_loss: 0.0000215246 val_loss: 0.1129095944\n",
      "Epoch 348 train_loss: 0.0000262394 val_loss: 0.1137370739\n",
      "Epoch 349 train_loss: 0.0000206436 val_loss: 0.1133256733\n",
      "Epoch 350 train_loss: 0.0000416070 val_loss: 0.1131210636\n",
      "Epoch 351 train_loss: 0.0000315272 val_loss: 0.1128287116\n",
      "Epoch 352 train_loss: 0.0000121577 val_loss: 0.1127874511\n",
      "Epoch 353 train_loss: 0.0000205989 val_loss: 0.1131181152\n",
      "Epoch 354 train_loss: 0.0000246609 val_loss: 0.1132258625\n",
      "Epoch 355 train_loss: 0.0000328323 val_loss: 0.1132160496\n",
      "Epoch 356 train_loss: 0.0000229026 val_loss: 0.1134288292\n",
      "Epoch 357 train_loss: 0.0000163674 val_loss: 0.1125960793\n",
      "Epoch 358 train_loss: 0.0000141137 val_loss: 0.1128283788\n",
      "Epoch 359 train_loss: 0.0000441154 val_loss: 0.1137313598\n",
      "Epoch 360 train_loss: 0.0000219789 val_loss: 0.1126551218\n",
      "Epoch 361 train_loss: 0.0000246307 val_loss: 0.1133929507\n",
      "Epoch 362 train_loss: 0.0001078856 val_loss: 0.1128502308\n",
      "Epoch 363 train_loss: 0.0000255570 val_loss: 0.1136699643\n",
      "Epoch 364 train_loss: 0.0000487124 val_loss: 0.1136184579\n",
      "Epoch 365 train_loss: 0.0000080868 val_loss: 0.1134057644\n",
      "Epoch 366 train_loss: 0.0000408965 val_loss: 0.1135309651\n",
      "Epoch 367 train_loss: 0.0000627391 val_loss: 0.1126493864\n",
      "Epoch 368 train_loss: 0.0001139571 val_loss: 0.1128526089\n",
      "Epoch 369 train_loss: 0.0000318171 val_loss: 0.1136215356\n",
      "Epoch 370 train_loss: 0.0000514738 val_loss: 0.1134575326\n",
      "Epoch 371 train_loss: 0.0000178496 val_loss: 0.1128587356\n",
      "Epoch 372 train_loss: 0.0000262248 val_loss: 0.1119994879\n",
      "Epoch 373 train_loss: 0.0000205242 val_loss: 0.1130345044\n",
      "Epoch 374 train_loss: 0.0000335893 val_loss: 0.1132412744\n",
      "Epoch 375 train_loss: 0.0000264626 val_loss: 0.1129867092\n",
      "Epoch 376 train_loss: 0.0000254745 val_loss: 0.1132430093\n",
      "Epoch 377 train_loss: 0.0000352532 val_loss: 0.1140261848\n",
      "Epoch 378 train_loss: 0.0000255383 val_loss: 0.1135947698\n",
      "Epoch 379 train_loss: 0.0000205810 val_loss: 0.1130143208\n",
      "Epoch 380 train_loss: 0.0000206326 val_loss: 0.1135748290\n",
      "Epoch 381 train_loss: 0.0000944585 val_loss: 0.1129671614\n",
      "Epoch 382 train_loss: 0.0000453613 val_loss: 0.1132161016\n",
      "Epoch 383 train_loss: 0.0000234310 val_loss: 0.1132010427\n",
      "Epoch 384 train_loss: 0.0000207497 val_loss: 0.1130463877\n",
      "Epoch 385 train_loss: 0.0000184673 val_loss: 0.1124689975\n",
      "Epoch 386 train_loss: 0.0000246462 val_loss: 0.1139764436\n",
      "Epoch 387 train_loss: 0.0000252799 val_loss: 0.1122227659\n",
      "Epoch 388 train_loss: 0.0000283016 val_loss: 0.1132078155\n",
      "Epoch 389 train_loss: 0.0000182002 val_loss: 0.1131593542\n",
      "Epoch 390 train_loss: 0.0000232654 val_loss: 0.1133072070\n",
      "Epoch 391 train_loss: 0.0000296558 val_loss: 0.1124799922\n",
      "Epoch 392 train_loss: 0.0000563679 val_loss: 0.1133137452\n",
      "Epoch 393 train_loss: 0.0000173878 val_loss: 0.1134608503\n",
      "Epoch 394 train_loss: 0.0000372911 val_loss: 0.1132602971\n",
      "Epoch 395 train_loss: 0.0000296706 val_loss: 0.1133874786\n",
      "Epoch 396 train_loss: 0.0000515618 val_loss: 0.1126993679\n",
      "Epoch 397 train_loss: 0.0000281898 val_loss: 0.1133058912\n",
      "Epoch 398 train_loss: 0.0000374236 val_loss: 0.1134913191\n",
      "Epoch 399 train_loss: 0.0000409626 val_loss: 0.1126059437\n",
      "Epoch 400 train_loss: 0.0000397327 val_loss: 0.1128437142\n",
      "Epoch 401 train_loss: 0.0000238460 val_loss: 0.1129781159\n",
      "Epoch 402 train_loss: 0.0000184408 val_loss: 0.1128236405\n",
      "Epoch 403 train_loss: 0.0000443383 val_loss: 0.1131086591\n",
      "Epoch 404 train_loss: 0.0000728761 val_loss: 0.1134567559\n",
      "Epoch 405 train_loss: 0.0000156805 val_loss: 0.1135342716\n",
      "Epoch 406 train_loss: 0.0000204974 val_loss: 0.1132755308\n",
      "Epoch 407 train_loss: 0.0000391598 val_loss: 0.1129629640\n",
      "Epoch 408 train_loss: 0.0000249714 val_loss: 0.1129516490\n",
      "Epoch 409 train_loss: 0.0000408447 val_loss: 0.1132937342\n",
      "Epoch 410 train_loss: 0.0000488001 val_loss: 0.1133296005\n",
      "Epoch 411 train_loss: 0.0000642259 val_loss: 0.1131041337\n",
      "Epoch 412 train_loss: 0.0000475239 val_loss: 0.1126437893\n",
      "Epoch 413 train_loss: 0.0000151676 val_loss: 0.1129632571\n",
      "Epoch 414 train_loss: 0.0000403393 val_loss: 0.1126085238\n",
      "Epoch 415 train_loss: 0.0000260379 val_loss: 0.1132364060\n",
      "Epoch 416 train_loss: 0.0000470296 val_loss: 0.1129748885\n",
      "Epoch 417 train_loss: 0.0000181601 val_loss: 0.1131506018\n",
      "Epoch 418 train_loss: 0.0000304104 val_loss: 0.1137163113\n",
      "Epoch 419 train_loss: 0.0000947738 val_loss: 0.1127048105\n",
      "Epoch 420 train_loss: 0.0000464170 val_loss: 0.1134426694\n",
      "Epoch 421 train_loss: 0.0000324463 val_loss: 0.1132657474\n",
      "Epoch 422 train_loss: 0.0000516814 val_loss: 0.1135986306\n",
      "Epoch 423 train_loss: 0.0000248629 val_loss: 0.1126737769\n",
      "Epoch 424 train_loss: 0.0000385161 val_loss: 0.1129226874\n",
      "Epoch 425 train_loss: 0.0000734882 val_loss: 0.1133143940\n",
      "Epoch 426 train_loss: 0.0000214448 val_loss: 0.1133211121\n",
      "Epoch 427 train_loss: 0.0000305395 val_loss: 0.1131245731\n",
      "Epoch 428 train_loss: 0.0000342619 val_loss: 0.1128397966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429 train_loss: 0.0000357993 val_loss: 0.1131759234\n",
      "Epoch 430 train_loss: 0.0000246265 val_loss: 0.1129685237\n",
      "Epoch 431 train_loss: 0.0000427490 val_loss: 0.1128790053\n",
      "Epoch 432 train_loss: 0.0000475187 val_loss: 0.1129786475\n",
      "Epoch 433 train_loss: 0.0000460389 val_loss: 0.1133520432\n",
      "Epoch 434 train_loss: 0.0000139612 val_loss: 0.1129440581\n",
      "Epoch 435 train_loss: 0.0000382416 val_loss: 0.1132571462\n",
      "Epoch 436 train_loss: 0.0000239989 val_loss: 0.1138019529\n",
      "Epoch 437 train_loss: 0.0000457348 val_loss: 0.1130309385\n",
      "Epoch 438 train_loss: 0.0000351062 val_loss: 0.1132676726\n",
      "Epoch 439 train_loss: 0.0001788344 val_loss: 0.1131688115\n",
      "Epoch 440 train_loss: 0.0000446386 val_loss: 0.1132474318\n",
      "Epoch 441 train_loss: 0.0000766489 val_loss: 0.1128010576\n",
      "Epoch 442 train_loss: 0.0000289588 val_loss: 0.1126022774\n",
      "Epoch 443 train_loss: 0.0000286189 val_loss: 0.1134549677\n",
      "Epoch 444 train_loss: 0.0000451036 val_loss: 0.1134876641\n",
      "Epoch 445 train_loss: 0.0000564353 val_loss: 0.1133746178\n",
      "Epoch 446 train_loss: 0.0000254216 val_loss: 0.1126746538\n",
      "Epoch 447 train_loss: 0.0000755746 val_loss: 0.1119797807\n",
      "Epoch 448 train_loss: 0.0000346849 val_loss: 0.1126155952\n",
      "Epoch 449 train_loss: 0.0000268586 val_loss: 0.1128004573\n",
      "Epoch 450 train_loss: 0.0000154280 val_loss: 0.1129214162\n",
      "Epoch 451 train_loss: 0.0000274223 val_loss: 0.1125537343\n",
      "Epoch 452 train_loss: 0.0000464543 val_loss: 0.1134786615\n",
      "Epoch 453 train_loss: 0.0000715741 val_loss: 0.1122566508\n",
      "Epoch 454 train_loss: 0.0000698625 val_loss: 0.1128752526\n",
      "Epoch 455 train_loss: 0.0000612738 val_loss: 0.1133659818\n",
      "Epoch 456 train_loss: 0.0000253836 val_loss: 0.1134074970\n",
      "Epoch 457 train_loss: 0.0000463313 val_loss: 0.1122461958\n",
      "Epoch 458 train_loss: 0.0000324966 val_loss: 0.1130724200\n",
      "Epoch 459 train_loss: 0.0000472299 val_loss: 0.1134442465\n",
      "Epoch 460 train_loss: 0.0000475116 val_loss: 0.1127842394\n",
      "Epoch 461 train_loss: 0.0000364097 val_loss: 0.1133354761\n",
      "Epoch 462 train_loss: 0.0000236855 val_loss: 0.1134571262\n",
      "Epoch 463 train_loss: 0.0000349153 val_loss: 0.1132248182\n",
      "Epoch 464 train_loss: 0.0000309279 val_loss: 0.1132690474\n",
      "Epoch 465 train_loss: 0.0000485726 val_loss: 0.1133542065\n",
      "Epoch 466 train_loss: 0.0000519857 val_loss: 0.1130815030\n",
      "Epoch 467 train_loss: 0.0000474954 val_loss: 0.1128608695\n",
      "Epoch 468 train_loss: 0.0000356326 val_loss: 0.1128426129\n",
      "Epoch 469 train_loss: 0.0000120486 val_loss: 0.1131908510\n",
      "Epoch 470 train_loss: 0.0000146685 val_loss: 0.1131011364\n",
      "Epoch 471 train_loss: 0.0000795220 val_loss: 0.1131144808\n",
      "Epoch 472 train_loss: 0.0000206744 val_loss: 0.1133717878\n",
      "Epoch 473 train_loss: 0.0000176235 val_loss: 0.1135404702\n",
      "Epoch 474 train_loss: 0.0000461282 val_loss: 0.1129347743\n",
      "Epoch 475 train_loss: 0.0000382414 val_loss: 0.1128793912\n",
      "Epoch 476 train_loss: 0.0000214725 val_loss: 0.1132062091\n",
      "Epoch 477 train_loss: 0.0000180614 val_loss: 0.1128661665\n",
      "Epoch 478 train_loss: 0.0001136355 val_loss: 0.1124888237\n",
      "Epoch 479 train_loss: 0.0000115103 val_loss: 0.1128411055\n",
      "Epoch 480 train_loss: 0.0000251128 val_loss: 0.1130231403\n",
      "Epoch 481 train_loss: 0.0000238665 val_loss: 0.1133625990\n",
      "Epoch 482 train_loss: 0.0000249804 val_loss: 0.1129789352\n",
      "Epoch 483 train_loss: 0.0000509809 val_loss: 0.1136563842\n",
      "Epoch 484 train_loss: 0.0000187868 val_loss: 0.1130216192\n",
      "Epoch 485 train_loss: 0.0000378992 val_loss: 0.1127394851\n",
      "Epoch 486 train_loss: 0.0000588135 val_loss: 0.1129732637\n",
      "Epoch 487 train_loss: 0.0000173568 val_loss: 0.1131451051\n",
      "Epoch 488 train_loss: 0.0000447512 val_loss: 0.1127943780\n",
      "Epoch 489 train_loss: 0.0000323399 val_loss: 0.1132709158\n",
      "Epoch 490 train_loss: 0.0000595870 val_loss: 0.1130575546\n",
      "Epoch 491 train_loss: 0.0000453300 val_loss: 0.1135055320\n",
      "Epoch 492 train_loss: 0.0000262834 val_loss: 0.1128695661\n",
      "Epoch 493 train_loss: 0.0000141445 val_loss: 0.1127980505\n",
      "Epoch 494 train_loss: 0.0000438014 val_loss: 0.1129833360\n",
      "Epoch 495 train_loss: 0.0000225036 val_loss: 0.1128749228\n",
      "Epoch 496 train_loss: 0.0000146117 val_loss: 0.1131214739\n",
      "Epoch 497 train_loss: 0.0000188406 val_loss: 0.1125498669\n",
      "Epoch 498 train_loss: 0.0000771151 val_loss: 0.1132766718\n",
      "Epoch 499 train_loss: 0.0000137066 val_loss: 0.1127418957\n",
      "Epoch 500 train_loss: 0.0000406289 val_loss: 0.1129160563\n"
     ]
    }
   ],
   "source": [
    "# training for Bi-LSTM\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    bi_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "        single_loss = criterion(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    bi_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            y_pred = model(inputs)\n",
    "            val_loss += criterion(y_pred, labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1} train_loss: {single_loss.item():.10f} val_loss: {val_loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e23777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000433352\n"
     ]
    }
   ],
   "source": [
    "# LSTM eval\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "\n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store predictions and actual values for further analysis if needed\n",
    "        predictions.append(y_pred.numpy())  # or y_pred.cpu().numpy() if using GPU\n",
    "        actuals.append(labels.numpy())\n",
    "\n",
    "# Calculate average loss over the test set\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.10f}')\n",
    "# Test Loss: 0.0004203604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cf4d6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# bi-LSTM eval\n",
    "bi_model.eval()\n",
    "bi_test_loss = 0.0\n",
    "bi_predictions = []\n",
    "bi_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y_pred = bi_model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "\n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store predictions and actual values for further analysis if needed\n",
    "        bi_predictions.append(y_pred.numpy())  # or y_pred.cpu().numpy() if using GPU\n",
    "        bi_actuals.append(labels.numpy())\n",
    "\n",
    "# Calculate average loss over the test set\n",
    "bi_test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {bi_test_loss:.10f}')\n",
    "# Test Loss: 0.0004203604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d5e9eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.tail of                     time       low      high      open     close       volume\n",
      "0    2023-11-16 12:00:00  36260.32  37332.32  37260.42  36316.17  6840.344833\n",
      "1    2023-11-16 18:00:00  35511.11  36400.18  36314.57  36161.15  7177.317828\n",
      "2    2023-11-17 00:00:00  36097.28  36674.75  36161.15  36363.27  2676.160827\n",
      "3    2023-11-17 06:00:00  36131.00  36473.75  36365.33  36382.68  1550.020928\n",
      "4    2023-11-17 12:00:00  35869.00  36831.99  36382.69  36508.83  4979.856742\n",
      "..                   ...       ...       ...       ...       ...          ...\n",
      "151  2023-12-24 06:00:00  43434.62  43721.81  43517.85  43655.99   637.606020\n",
      "152  2023-12-24 12:00:00  43578.33  43901.70  43658.10  43654.11   882.832758\n",
      "153  2023-12-24 18:00:00  42614.17  43719.56  43656.52  43025.03  2143.310127\n",
      "154  2023-12-25 00:00:00  42755.35  43241.15  43025.02  43231.21  1245.264334\n",
      "155  2023-12-25 06:00:00  43056.87  43339.64  43233.08  43188.37   807.055913\n",
      "\n",
      "[156 rows x 6 columns]>\n",
      "Predicted next 5 closing prices: [43150.133, 43132.695, 43097.816, 43067.355, 43001.484, 42977.594, 42925.934, 42884.973, 42845.54, 42804.566, 42766.73, 42726.867, 42690.125, 42651.71, 42614.742, 42577.96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/190836376.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n"
     ]
    }
   ],
   "source": [
    "# inference for LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and normalize the inference data\n",
    "inf_data = pd.read_csv(\"inf.csv\")\n",
    "print(inf_data.tail)\n",
    "\n",
    "# Fit a separate scaler for the 'close' feature\n",
    "close_scaler = MinMaxScaler()\n",
    "inf_data['close'] = close_scaler.fit_transform(inf_data[['close']])\n",
    "\n",
    "# Prepare the initial recent data for prediction\n",
    "window_size = 10  # Adjust based on your model's training\n",
    "recent_data = inf_data['close'][-window_size:].values.reshape(1, window_size, 1)  # Reshape for single batch, single feature\n",
    "\n",
    "# Recursively predict next 5 time steps\n",
    "num_predictions = 16\n",
    "predictions = []\n",
    "bi_predictions = []\n",
    "\n",
    "for _ in range(num_predictions):\n",
    "    # Convert recent data to tensor\n",
    "    recent_data_tensor = torch.FloatTensor(recent_data)\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_next_step = model(recent_data_tensor).numpy()  # Get the model's prediction for the next step\n",
    "\n",
    "    # Inverse transform to get actual price scale for the predicted step\n",
    "    predicted_close_price = close_scaler.inverse_transform(predicted_next_step.reshape(-1, 1))[0, 0]\n",
    "    predictions.append(predicted_close_price)\n",
    "\n",
    "    # Update the recent data with the predicted value\n",
    "    recent_data = np.roll(recent_data, -1, axis=1)  # Shift everything one step to the left\n",
    "    recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
    "\n",
    "# predictions now contains the 5 sequential forecasted values\n",
    "print(\"Predicted next 5 closing prices:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "25b993d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next 5 closing prices: [35652.38, 35652.43, 35652.37, 35652.3, 35652.258, 35652.246, 35652.246, 35652.254, 35652.254, 35652.254, 35652.254, 35652.254, 35652.254, 35652.254, 35652.254, 35652.254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_68161/923226228.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n"
     ]
    }
   ],
   "source": [
    "# inference for bi-LSTM\n",
    "# Fit a separate scaler for the 'close' feature\n",
    "bi_predictions = []\n",
    "\n",
    "for _ in range(num_predictions):\n",
    "    # Convert recent data to tensor\n",
    "    recent_data_tensor = torch.FloatTensor(recent_data)\n",
    "\n",
    "    # Perform inference\n",
    "    bi_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_next_step = bi_model(recent_data_tensor).numpy()  # Get the model's prediction for the next step\n",
    "\n",
    "    # Inverse transform to get actual price scale for the predicted step\n",
    "    predicted_close_price = close_scaler.inverse_transform(predicted_next_step.reshape(-1, 1))[0, 0]\n",
    "    bi_predictions.append(predicted_close_price)\n",
    "\n",
    "    # Update the recent data with the predicted value\n",
    "    recent_data = np.roll(recent_data, -1, axis=1)  # Shift everything one step to the left\n",
    "    recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
    "\n",
    "# predictions now contains the 5 sequential forecasted values\n",
    "print(\"Predicted next 5 closing prices:\", bi_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crypto_model)",
   "language": "python",
   "name": "crypto_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
