{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23c5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83847b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      time       low      high      open     close       volume\n",
       "0     2015-11-30 12:00:00    367.50    379.42    378.16    373.00  3031.105717\n",
       "1     2015-11-30 18:00:00    372.25    378.55    373.00    376.86  2652.465161\n",
       "2     2015-12-01 00:00:00    375.80    378.94    376.88    378.01  1695.388592\n",
       "3     2015-12-01 06:00:00    354.60    378.54    378.00    361.20  2682.538466\n",
       "4     2015-12-01 12:00:00    355.00    364.86    361.21    360.14  2864.005011\n",
       "...                   ...       ...       ...       ...       ...          ...\n",
       "11622 2023-11-14 06:00:00  36249.50  36756.16  36642.00  36272.73  1455.611439\n",
       "11623 2023-11-14 12:00:00  35855.00  36708.43  36272.63  36088.57  5289.680943\n",
       "11624 2023-11-14 18:00:00  34758.64  36122.76  36090.42  35554.09  7511.702326\n",
       "11625 2023-11-15 00:00:00  35358.45  35670.90  35554.10  35610.51  1929.755005\n",
       "11626 2023-11-15 06:00:00  35533.29  36293.22  35610.08  36253.78  1693.626865\n",
       "\n",
       "[11627 rows x 6 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    return df\n",
    "\n",
    "df = clean(\"train.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b37c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      time       low      high      open     close    volume\n",
       "0     2015-11-30 12:00:00  0.005457  0.000341  0.000368  0.000293  0.029584\n",
       "1     2015-11-30 18:00:00  0.005527  0.000328  0.000293  0.000349  0.025770\n",
       "2     2015-12-01 00:00:00  0.005580  0.000334  0.000350  0.000366  0.016130\n",
       "3     2015-12-01 06:00:00  0.005265  0.000328  0.000366  0.000120  0.026073\n",
       "4     2015-12-01 12:00:00  0.005271  0.000129  0.000120  0.000104  0.027901\n",
       "...                   ...       ...       ...       ...       ...       ...\n",
       "11622 2023-11-14 06:00:00  0.538342  0.530274  0.531431  0.526014  0.013715\n",
       "11623 2023-11-14 12:00:00  0.532483  0.529579  0.526022  0.523317  0.052333\n",
       "11624 2023-11-14 18:00:00  0.516201  0.521047  0.523353  0.515490  0.074715\n",
       "11625 2023-11-15 00:00:00  0.525109  0.514464  0.515499  0.516316  0.018491\n",
       "11626 2023-11-15 06:00:00  0.527706  0.523530  0.516319  0.525737  0.016112\n",
       "\n",
       "[11627 rows x 6 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = ['low', 'high', 'open', 'close', 'volume']  # adjust with your column names\n",
    "    df[cols] = scaler.fit_transform(df[cols])\n",
    "    return df, scaler\n",
    "\n",
    "def reverse_norm(pred, scaler):\n",
    "    predictions = pred.reshape(-1, 1)\n",
    "    real_pred = scaler.inverse_transform(predictions)\n",
    "    return real_pred\n",
    "    \n",
    "norm, scaler = normalize(df)\n",
    "norm.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5208f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, window_size):\n",
    "    sequences = []\n",
    "    df_size = len(df)\n",
    "    for i in range(df_size - window_size):\n",
    "        # Here, i: start of the sequence, i + window_size: end of the sequence\n",
    "        sequence = df[i:i + window_size]\n",
    "        label = df[i + window_size:i + window_size + 1]  # next value to be predicted\n",
    "        sequences.append((sequence, label))\n",
    "    return sequences\n",
    "\n",
    "window_size = 10  # This is just an example value\n",
    "\n",
    "# Assuming 'close' is what you want to predict\n",
    "sequences = create_sequences(norm['close'].values, window_size)\n",
    "sequences = [(torch.FloatTensor(seq), torch.FloatTensor(lbl)) for seq, lbl in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, test_size=0.4, val_size=0.5, random_state=42):\n",
    "    X, y = zip(*sequences)  # No change here, but X and y are already tensors\n",
    "    X = torch.stack(X)  # Stack all sequence tensors\n",
    "    y = torch.stack(y).squeeze()  # Stack all label tensors and remove extra dimension\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = torch.randperm(X.size(0))\n",
    "    X, y = X[indices], y[indices]\n",
    "\n",
    "    # Calculate split sizes\n",
    "    train_end = int(X.size(0) * (1 - test_size))\n",
    "    val_end = int(train_end * (1 - val_size))\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp = X[:train_end], X[train_end:]\n",
    "    y_train, y_temp = y[:train_end], y[train_end:]\n",
    "    X_val, X_test = X_temp[:val_end], X_temp[val_end:]\n",
    "    y_val, y_test = y_temp[:val_end], y_temp[val_end:]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898f9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you have already run the modified split_sequences function\n",
    "# and have X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create TensorDataset\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64  # Adjust as necessary\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60bf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # Setting bidirectional=True doubles the output feature dimension \n",
    "        # because it concatenates the hidden states from both directions\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Since the LSTM is bidirectional, we need to double the input feature dimension\n",
    "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # lstm_out shape is (batch, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), -1, input_size))\n",
    "        \n",
    "        # We take the last time step's output from both directions\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "num_features = 5\n",
    "input_size = num_features  # number of features in your input\n",
    "hidden_layer_size = 180  # number of features in hidden state\n",
    "output_size = 1  # predict one feature\n",
    "\n",
    "model = BiLSTM(input_size, hidden_layer_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf91753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/loganwork/anaconda3/envs/crypto_model/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.0002138336 val_loss: 0.1033149486\n",
      "Epoch 2 train_loss: 0.0001650601 val_loss: 0.1051297264\n",
      "Epoch 3 train_loss: 0.0001400913 val_loss: 0.1039687841\n",
      "Epoch 4 train_loss: 0.0001309434 val_loss: 0.1047549744\n",
      "Epoch 5 train_loss: 0.0000737583 val_loss: 0.1036933917\n",
      "Epoch 6 train_loss: 0.0000410145 val_loss: 0.1051873284\n",
      "Epoch 7 train_loss: 0.0000535088 val_loss: 0.1037446519\n",
      "Epoch 8 train_loss: 0.0000598358 val_loss: 0.1040683583\n",
      "Epoch 9 train_loss: 0.0000948525 val_loss: 0.1054801428\n",
      "Epoch 10 train_loss: 0.0000612892 val_loss: 0.1043496766\n",
      "Epoch 11 train_loss: 0.0000644349 val_loss: 0.1049921801\n",
      "Epoch 12 train_loss: 0.0001009529 val_loss: 0.1037126994\n",
      "Epoch 13 train_loss: 0.0002575785 val_loss: 0.1045449342\n",
      "Epoch 14 train_loss: 0.0000852729 val_loss: 0.1034034552\n",
      "Epoch 15 train_loss: 0.0000473852 val_loss: 0.1050553957\n",
      "Epoch 16 train_loss: 0.0001252121 val_loss: 0.1048035968\n",
      "Epoch 17 train_loss: 0.0001397037 val_loss: 0.1039989091\n",
      "Epoch 18 train_loss: 0.0000708899 val_loss: 0.1050157871\n",
      "Epoch 19 train_loss: 0.0000124353 val_loss: 0.1048930528\n",
      "Epoch 20 train_loss: 0.0000836657 val_loss: 0.1034624888\n",
      "Epoch 21 train_loss: 0.0000656117 val_loss: 0.1036999595\n",
      "Epoch 22 train_loss: 0.0000363909 val_loss: 0.1049544554\n",
      "Epoch 23 train_loss: 0.0001247795 val_loss: 0.1039349678\n",
      "Epoch 24 train_loss: 0.0000648174 val_loss: 0.1038544362\n",
      "Epoch 25 train_loss: 0.0000876382 val_loss: 0.1046358107\n",
      "Epoch 26 train_loss: 0.0000498301 val_loss: 0.1047066380\n",
      "Epoch 27 train_loss: 0.0000506198 val_loss: 0.1038970588\n",
      "Epoch 28 train_loss: 0.0003397825 val_loss: 0.1034060904\n",
      "Epoch 29 train_loss: 0.0001227737 val_loss: 0.1054550023\n",
      "Epoch 30 train_loss: 0.0000129776 val_loss: 0.1036974014\n",
      "Epoch 31 train_loss: 0.0000360572 val_loss: 0.1046712558\n",
      "Epoch 32 train_loss: 0.0000895529 val_loss: 0.1038792062\n",
      "Epoch 33 train_loss: 0.0000434151 val_loss: 0.1041480956\n",
      "Epoch 34 train_loss: 0.0000536288 val_loss: 0.1053444453\n",
      "Epoch 35 train_loss: 0.0000788601 val_loss: 0.1045651544\n",
      "Epoch 36 train_loss: 0.0000303756 val_loss: 0.1046223788\n",
      "Epoch 37 train_loss: 0.0000308697 val_loss: 0.1040155441\n",
      "Epoch 38 train_loss: 0.0000763876 val_loss: 0.1050893257\n",
      "Epoch 39 train_loss: 0.0000848610 val_loss: 0.1037223773\n",
      "Epoch 40 train_loss: 0.0000155351 val_loss: 0.1048189920\n",
      "Epoch 41 train_loss: 0.0000204463 val_loss: 0.1047185249\n",
      "Epoch 42 train_loss: 0.0000853383 val_loss: 0.1036506253\n",
      "Epoch 43 train_loss: 0.0000409035 val_loss: 0.1049565745\n",
      "Epoch 44 train_loss: 0.0000586529 val_loss: 0.1046064452\n",
      "Epoch 45 train_loss: 0.0000195112 val_loss: 0.1043431961\n",
      "Epoch 46 train_loss: 0.0000900825 val_loss: 0.1041447495\n",
      "Epoch 47 train_loss: 0.0000836123 val_loss: 0.1038532340\n",
      "Epoch 48 train_loss: 0.0000248714 val_loss: 0.1043146834\n",
      "Epoch 49 train_loss: 0.0000776004 val_loss: 0.1055259995\n",
      "Epoch 50 train_loss: 0.0000437041 val_loss: 0.1042902677\n",
      "Epoch 51 train_loss: 0.0000610113 val_loss: 0.1054654911\n",
      "Epoch 52 train_loss: 0.0000340847 val_loss: 0.1036268674\n",
      "Epoch 53 train_loss: 0.0000744285 val_loss: 0.1044620500\n",
      "Epoch 54 train_loss: 0.0000579541 val_loss: 0.1049811528\n",
      "Epoch 55 train_loss: 0.0000161956 val_loss: 0.1042223141\n",
      "Epoch 56 train_loss: 0.0000166506 val_loss: 0.1046924983\n",
      "Epoch 57 train_loss: 0.0000821774 val_loss: 0.1038525365\n",
      "Epoch 58 train_loss: 0.0000246409 val_loss: 0.1033699145\n",
      "Epoch 59 train_loss: 0.0000334678 val_loss: 0.1028929951\n",
      "Epoch 60 train_loss: 0.0000349152 val_loss: 0.1039467684\n",
      "Epoch 61 train_loss: 0.0000799531 val_loss: 0.1047817523\n",
      "Epoch 62 train_loss: 0.0001033305 val_loss: 0.1033178266\n",
      "Epoch 63 train_loss: 0.0000177191 val_loss: 0.1045168824\n",
      "Epoch 64 train_loss: 0.0000706332 val_loss: 0.1043371542\n",
      "Epoch 65 train_loss: 0.0000170452 val_loss: 0.1048241600\n",
      "Epoch 66 train_loss: 0.0000419097 val_loss: 0.1045845756\n",
      "Epoch 67 train_loss: 0.0000236804 val_loss: 0.1041047849\n",
      "Epoch 68 train_loss: 0.0000337063 val_loss: 0.1044949611\n",
      "Epoch 69 train_loss: 0.0000762784 val_loss: 0.1057937082\n",
      "Epoch 70 train_loss: 0.0000485146 val_loss: 0.1043679798\n",
      "Epoch 71 train_loss: 0.0000374237 val_loss: 0.1049134315\n",
      "Epoch 72 train_loss: 0.0000967160 val_loss: 0.1040100551\n",
      "Epoch 73 train_loss: 0.0000479573 val_loss: 0.1043848716\n",
      "Epoch 74 train_loss: 0.0000596782 val_loss: 0.1042225136\n",
      "Epoch 75 train_loss: 0.0000395447 val_loss: 0.1039156448\n",
      "Epoch 76 train_loss: 0.0000119419 val_loss: 0.1037020250\n",
      "Epoch 77 train_loss: 0.0000200023 val_loss: 0.1043052094\n",
      "Epoch 78 train_loss: 0.0000538878 val_loss: 0.1053091781\n",
      "Epoch 79 train_loss: 0.0000570707 val_loss: 0.1056195699\n",
      "Epoch 80 train_loss: 0.0000884215 val_loss: 0.1040399836\n",
      "Epoch 81 train_loss: 0.0000409747 val_loss: 0.1039993520\n",
      "Epoch 82 train_loss: 0.0000730815 val_loss: 0.1027990300\n",
      "Epoch 83 train_loss: 0.0000559631 val_loss: 0.1045730935\n",
      "Epoch 84 train_loss: 0.0000315518 val_loss: 0.1049458179\n",
      "Epoch 85 train_loss: 0.0000792889 val_loss: 0.1041025137\n",
      "Epoch 86 train_loss: 0.0000430401 val_loss: 0.1047542758\n",
      "Epoch 87 train_loss: 0.0000194553 val_loss: 0.1040969746\n",
      "Epoch 88 train_loss: 0.0001117237 val_loss: 0.1041163274\n",
      "Epoch 89 train_loss: 0.0000545007 val_loss: 0.1042507823\n",
      "Epoch 90 train_loss: 0.0000441904 val_loss: 0.1032927613\n",
      "Epoch 91 train_loss: 0.0000703173 val_loss: 0.1052851249\n",
      "Epoch 92 train_loss: 0.0000814896 val_loss: 0.1034782672\n",
      "Epoch 93 train_loss: 0.0000585940 val_loss: 0.1053938526\n",
      "Epoch 94 train_loss: 0.0000131685 val_loss: 0.1046666446\n",
      "Epoch 95 train_loss: 0.0000169204 val_loss: 0.1041648766\n",
      "Epoch 96 train_loss: 0.0000712316 val_loss: 0.1034773172\n",
      "Epoch 97 train_loss: 0.0000106638 val_loss: 0.1031125692\n",
      "Epoch 98 train_loss: 0.0000040607 val_loss: 0.1045006140\n",
      "Epoch 99 train_loss: 0.0000484232 val_loss: 0.1045373945\n",
      "Epoch 100 train_loss: 0.0000185973 val_loss: 0.1044954117\n",
      "Epoch 101 train_loss: 0.0000342221 val_loss: 0.1041477411\n",
      "Epoch 102 train_loss: 0.0000509197 val_loss: 0.1041002191\n",
      "Epoch 103 train_loss: 0.0000178227 val_loss: 0.1053423531\n",
      "Epoch 104 train_loss: 0.0000300301 val_loss: 0.1034584706\n",
      "Epoch 105 train_loss: 0.0000290157 val_loss: 0.1047024092\n",
      "Epoch 106 train_loss: 0.0000392988 val_loss: 0.1044424208\n",
      "Epoch 107 train_loss: 0.0000517630 val_loss: 0.1041526286\n",
      "Epoch 108 train_loss: 0.0000203904 val_loss: 0.1035331283\n",
      "Epoch 109 train_loss: 0.0000262715 val_loss: 0.1045815034\n",
      "Epoch 110 train_loss: 0.0000510213 val_loss: 0.1041742204\n",
      "Epoch 111 train_loss: 0.0000265360 val_loss: 0.1049711076\n",
      "Epoch 112 train_loss: 0.0000512537 val_loss: 0.1048719515\n",
      "Epoch 113 train_loss: 0.0000552777 val_loss: 0.1053163505\n",
      "Epoch 114 train_loss: 0.0000351912 val_loss: 0.1050798701\n",
      "Epoch 115 train_loss: 0.0000123363 val_loss: 0.1044831964\n",
      "Epoch 116 train_loss: 0.0000190042 val_loss: 0.1052616612\n",
      "Epoch 117 train_loss: 0.0000299445 val_loss: 0.1058917455\n",
      "Epoch 118 train_loss: 0.0000244535 val_loss: 0.1050334258\n",
      "Epoch 119 train_loss: 0.0000349083 val_loss: 0.1040247980\n",
      "Epoch 120 train_loss: 0.0000376887 val_loss: 0.1048239575\n",
      "Epoch 121 train_loss: 0.0000567196 val_loss: 0.1042481465\n",
      "Epoch 122 train_loss: 0.0000192806 val_loss: 0.1037526622\n",
      "Epoch 123 train_loss: 0.0000361492 val_loss: 0.1053766360\n",
      "Epoch 124 train_loss: 0.0000668489 val_loss: 0.1056677105\n",
      "Epoch 125 train_loss: 0.0000081791 val_loss: 0.1034555698\n",
      "Epoch 126 train_loss: 0.0000723032 val_loss: 0.1052876619\n",
      "Epoch 127 train_loss: 0.0000460522 val_loss: 0.1053136742\n",
      "Epoch 128 train_loss: 0.0000682711 val_loss: 0.1043205817\n",
      "Epoch 129 train_loss: 0.0000877356 val_loss: 0.1043652306\n",
      "Epoch 130 train_loss: 0.0000544235 val_loss: 0.1037318842\n",
      "Epoch 131 train_loss: 0.0000726389 val_loss: 0.1041720652\n",
      "Epoch 132 train_loss: 0.0000832237 val_loss: 0.1052770043\n",
      "Epoch 133 train_loss: 0.0000829100 val_loss: 0.1050469726\n",
      "Epoch 134 train_loss: 0.0000140713 val_loss: 0.1034735250\n",
      "Epoch 135 train_loss: 0.0000494550 val_loss: 0.1043601459\n",
      "Epoch 136 train_loss: 0.0000723623 val_loss: 0.1052239501\n",
      "Epoch 137 train_loss: 0.0000123585 val_loss: 0.1048688572\n",
      "Epoch 138 train_loss: 0.0000527290 val_loss: 0.1037484758\n",
      "Epoch 139 train_loss: 0.0000308859 val_loss: 0.1038518818\n",
      "Epoch 140 train_loss: 0.0000703114 val_loss: 0.1047463817\n",
      "Epoch 141 train_loss: 0.0000188468 val_loss: 0.1036391941\n",
      "Epoch 142 train_loss: 0.0000203941 val_loss: 0.1047753293\n",
      "Epoch 143 train_loss: 0.0000358994 val_loss: 0.1037650278\n",
      "Epoch 144 train_loss: 0.0000686273 val_loss: 0.1053552840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 train_loss: 0.0000071465 val_loss: 0.1040665324\n",
      "Epoch 146 train_loss: 0.0000776760 val_loss: 0.1052016245\n",
      "Epoch 147 train_loss: 0.0000541024 val_loss: 0.1029275522\n",
      "Epoch 148 train_loss: 0.0000535775 val_loss: 0.1044841012\n",
      "Epoch 149 train_loss: 0.0000406462 val_loss: 0.1047809406\n",
      "Epoch 150 train_loss: 0.0000135275 val_loss: 0.1035545261\n",
      "Epoch 151 train_loss: 0.0000205243 val_loss: 0.1045386260\n",
      "Epoch 152 train_loss: 0.0000213462 val_loss: 0.1042629262\n",
      "Epoch 153 train_loss: 0.0000578603 val_loss: 0.1048055375\n",
      "Epoch 154 train_loss: 0.0000550599 val_loss: 0.1047439008\n",
      "Epoch 155 train_loss: 0.0000118497 val_loss: 0.1039134580\n",
      "Epoch 156 train_loss: 0.0000580654 val_loss: 0.1035440393\n",
      "Epoch 157 train_loss: 0.0000546161 val_loss: 0.1034593135\n",
      "Epoch 158 train_loss: 0.0000237207 val_loss: 0.1041427101\n",
      "Epoch 159 train_loss: 0.0000908975 val_loss: 0.1045544962\n",
      "Epoch 160 train_loss: 0.0000402656 val_loss: 0.1039707998\n",
      "Epoch 161 train_loss: 0.0002626177 val_loss: 0.1029353787\n",
      "Epoch 162 train_loss: 0.0000459551 val_loss: 0.1039301074\n",
      "Epoch 163 train_loss: 0.0001360489 val_loss: 0.1043735392\n",
      "Epoch 164 train_loss: 0.0000830229 val_loss: 0.1040043540\n",
      "Epoch 165 train_loss: 0.0000334604 val_loss: 0.1038729922\n",
      "Epoch 166 train_loss: 0.0000271327 val_loss: 0.1034415513\n",
      "Epoch 167 train_loss: 0.0000783148 val_loss: 0.1039933639\n",
      "Epoch 168 train_loss: 0.0000410866 val_loss: 0.1046729884\n",
      "Epoch 169 train_loss: 0.0000175284 val_loss: 0.1036776911\n",
      "Epoch 170 train_loss: 0.0000309140 val_loss: 0.1045867638\n",
      "Epoch 171 train_loss: 0.0000535633 val_loss: 0.1038952320\n",
      "Epoch 172 train_loss: 0.0000414499 val_loss: 0.1054014260\n",
      "Epoch 173 train_loss: 0.0000210576 val_loss: 0.1043658690\n",
      "Epoch 174 train_loss: 0.0000377332 val_loss: 0.1042822398\n",
      "Epoch 175 train_loss: 0.0000350571 val_loss: 0.1042451065\n",
      "Epoch 176 train_loss: 0.0000234377 val_loss: 0.1035736730\n",
      "Epoch 177 train_loss: 0.0000242286 val_loss: 0.1036642995\n",
      "Epoch 178 train_loss: 0.0000383519 val_loss: 0.1057685486\n",
      "Epoch 179 train_loss: 0.0000420256 val_loss: 0.1045538655\n",
      "Epoch 180 train_loss: 0.0000521228 val_loss: 0.1044720638\n",
      "Epoch 181 train_loss: 0.0000557463 val_loss: 0.1053287260\n",
      "Epoch 182 train_loss: 0.0000776637 val_loss: 0.1047086769\n",
      "Epoch 183 train_loss: 0.0000207419 val_loss: 0.1041493014\n",
      "Epoch 184 train_loss: 0.0000313428 val_loss: 0.1040697214\n",
      "Epoch 185 train_loss: 0.0000344987 val_loss: 0.1045027368\n",
      "Epoch 186 train_loss: 0.0000834116 val_loss: 0.1044387462\n",
      "Epoch 187 train_loss: 0.0000935792 val_loss: 0.1050022653\n",
      "Epoch 188 train_loss: 0.0000455446 val_loss: 0.1034626480\n",
      "Epoch 189 train_loss: 0.0000326824 val_loss: 0.1045316169\n",
      "Epoch 190 train_loss: 0.0000328730 val_loss: 0.1043858176\n",
      "Epoch 191 train_loss: 0.0000425808 val_loss: 0.1041174379\n",
      "Epoch 192 train_loss: 0.0000563307 val_loss: 0.1039346083\n",
      "Epoch 193 train_loss: 0.0000258842 val_loss: 0.1037870127\n",
      "Epoch 194 train_loss: 0.0000200635 val_loss: 0.1051465430\n",
      "Epoch 195 train_loss: 0.0000498636 val_loss: 0.1036323194\n",
      "Epoch 196 train_loss: 0.0000328347 val_loss: 0.1046840783\n",
      "Epoch 197 train_loss: 0.0000171030 val_loss: 0.1049241963\n",
      "Epoch 198 train_loss: 0.0000354269 val_loss: 0.1040342101\n",
      "Epoch 199 train_loss: 0.0000672622 val_loss: 0.1045482437\n",
      "Epoch 200 train_loss: 0.0000639884 val_loss: 0.1036859144\n",
      "Epoch 201 train_loss: 0.0000469033 val_loss: 0.1044582363\n",
      "Epoch 202 train_loss: 0.0000670385 val_loss: 0.1036250278\n",
      "Epoch 203 train_loss: 0.0000413411 val_loss: 0.1050976002\n",
      "Epoch 204 train_loss: 0.0000785319 val_loss: 0.1053536217\n",
      "Epoch 205 train_loss: 0.0000382361 val_loss: 0.1038643437\n",
      "Epoch 206 train_loss: 0.0000231325 val_loss: 0.1040353190\n",
      "Epoch 207 train_loss: 0.0000274561 val_loss: 0.1040098327\n",
      "Epoch 208 train_loss: 0.0000245290 val_loss: 0.1047432503\n",
      "Epoch 209 train_loss: 0.0001488326 val_loss: 0.1046006759\n",
      "Epoch 210 train_loss: 0.0000364904 val_loss: 0.1051056978\n",
      "Epoch 211 train_loss: 0.0000538583 val_loss: 0.1032566704\n",
      "Epoch 212 train_loss: 0.0000700152 val_loss: 0.1045249755\n",
      "Epoch 213 train_loss: 0.0000170886 val_loss: 0.1044238830\n",
      "Epoch 214 train_loss: 0.0000350780 val_loss: 0.1043884529\n",
      "Epoch 215 train_loss: 0.0000519888 val_loss: 0.1037560093\n",
      "Epoch 216 train_loss: 0.0000665012 val_loss: 0.1042574272\n",
      "Epoch 217 train_loss: 0.0000341930 val_loss: 0.1042231054\n",
      "Epoch 218 train_loss: 0.0000487177 val_loss: 0.1045027410\n",
      "Epoch 219 train_loss: 0.0000735965 val_loss: 0.1048901172\n",
      "Epoch 220 train_loss: 0.0000445783 val_loss: 0.1038790969\n",
      "Epoch 221 train_loss: 0.0000508560 val_loss: 0.1050619950\n",
      "Epoch 222 train_loss: 0.0000190200 val_loss: 0.1042537973\n",
      "Epoch 223 train_loss: 0.0000387505 val_loss: 0.1038744540\n",
      "Epoch 224 train_loss: 0.0000226320 val_loss: 0.1050787496\n",
      "Epoch 225 train_loss: 0.0000545381 val_loss: 0.1044085268\n",
      "Epoch 226 train_loss: 0.0000482151 val_loss: 0.1038700300\n",
      "Epoch 227 train_loss: 0.0000335241 val_loss: 0.1054769521\n",
      "Epoch 228 train_loss: 0.0000421337 val_loss: 0.1041376278\n",
      "Epoch 229 train_loss: 0.0000259514 val_loss: 0.1033100724\n",
      "Epoch 230 train_loss: 0.0000655910 val_loss: 0.1036868479\n",
      "Epoch 231 train_loss: 0.0000529768 val_loss: 0.1053950831\n",
      "Epoch 232 train_loss: 0.0000479190 val_loss: 0.1033211266\n",
      "Epoch 233 train_loss: 0.0000071738 val_loss: 0.1046406830\n",
      "Epoch 234 train_loss: 0.0000297918 val_loss: 0.1046094403\n",
      "Epoch 235 train_loss: 0.0000429443 val_loss: 0.1039886964\n",
      "Epoch 236 train_loss: 0.0000391210 val_loss: 0.1048838380\n",
      "Epoch 237 train_loss: 0.0000451305 val_loss: 0.1031954398\n",
      "Epoch 238 train_loss: 0.0000830136 val_loss: 0.1034318962\n",
      "Epoch 239 train_loss: 0.0000319998 val_loss: 0.1046125455\n",
      "Epoch 240 train_loss: 0.0000234709 val_loss: 0.1051213487\n",
      "Epoch 241 train_loss: 0.0000303263 val_loss: 0.1033590897\n",
      "Epoch 242 train_loss: 0.0000260844 val_loss: 0.1033352688\n",
      "Epoch 243 train_loss: 0.0000316797 val_loss: 0.1057831146\n",
      "Epoch 244 train_loss: 0.0000294762 val_loss: 0.1043287017\n",
      "Epoch 245 train_loss: 0.0000234103 val_loss: 0.1040146524\n",
      "Epoch 246 train_loss: 0.0001139200 val_loss: 0.1044096605\n",
      "Epoch 247 train_loss: 0.0000431044 val_loss: 0.1047545693\n",
      "Epoch 248 train_loss: 0.0000124595 val_loss: 0.1039354152\n",
      "Epoch 249 train_loss: 0.0000953496 val_loss: 0.1042612786\n",
      "Epoch 250 train_loss: 0.0000283288 val_loss: 0.1048981029\n",
      "Epoch 251 train_loss: 0.0000460514 val_loss: 0.1046863028\n",
      "Epoch 252 train_loss: 0.0000194758 val_loss: 0.1039920155\n",
      "Epoch 253 train_loss: 0.0000509252 val_loss: 0.1045736299\n",
      "Epoch 254 train_loss: 0.0000380688 val_loss: 0.1049659459\n",
      "Epoch 255 train_loss: 0.0000454927 val_loss: 0.1039165471\n",
      "Epoch 256 train_loss: 0.0000256434 val_loss: 0.1045495176\n",
      "Epoch 257 train_loss: 0.0000994639 val_loss: 0.1034407641\n",
      "Epoch 258 train_loss: 0.0000166176 val_loss: 0.1041102531\n",
      "Epoch 259 train_loss: 0.0000296017 val_loss: 0.1052825883\n",
      "Epoch 260 train_loss: 0.0000765261 val_loss: 0.1038111560\n",
      "Epoch 261 train_loss: 0.0000866088 val_loss: 0.1043379124\n",
      "Epoch 262 train_loss: 0.0000290458 val_loss: 0.1041827465\n",
      "Epoch 263 train_loss: 0.0000437255 val_loss: 0.1035421711\n",
      "Epoch 264 train_loss: 0.0000390156 val_loss: 0.1037395988\n",
      "Epoch 265 train_loss: 0.0000540727 val_loss: 0.1039932130\n",
      "Epoch 266 train_loss: 0.0000080136 val_loss: 0.1044612585\n",
      "Epoch 267 train_loss: 0.0000263533 val_loss: 0.1035420620\n",
      "Epoch 268 train_loss: 0.0000537769 val_loss: 0.1038936342\n",
      "Epoch 269 train_loss: 0.0000100940 val_loss: 0.1033778156\n",
      "Epoch 270 train_loss: 0.0000571398 val_loss: 0.1042510970\n",
      "Epoch 271 train_loss: 0.0000431234 val_loss: 0.1037615207\n",
      "Epoch 272 train_loss: 0.0000316583 val_loss: 0.1046529855\n",
      "Epoch 273 train_loss: 0.0000704466 val_loss: 0.1042197460\n",
      "Epoch 274 train_loss: 0.0000911592 val_loss: 0.1040877627\n",
      "Epoch 275 train_loss: 0.0000111353 val_loss: 0.1045522129\n",
      "Epoch 276 train_loss: 0.0000614373 val_loss: 0.1041842780\n",
      "Epoch 277 train_loss: 0.0000365697 val_loss: 0.1050389446\n",
      "Epoch 278 train_loss: 0.0000288396 val_loss: 0.1048149319\n",
      "Epoch 279 train_loss: 0.0000341793 val_loss: 0.1041192989\n",
      "Epoch 280 train_loss: 0.0000325726 val_loss: 0.1042383823\n",
      "Epoch 281 train_loss: 0.0000448704 val_loss: 0.1033139922\n",
      "Epoch 282 train_loss: 0.0000181414 val_loss: 0.1042368764\n",
      "Epoch 283 train_loss: 0.0000467483 val_loss: 0.1032921691\n",
      "Epoch 284 train_loss: 0.0000148778 val_loss: 0.1040138654\n",
      "Epoch 285 train_loss: 0.0000272928 val_loss: 0.1054753602\n",
      "Epoch 286 train_loss: 0.0000334755 val_loss: 0.1043584396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287 train_loss: 0.0001220944 val_loss: 0.1053543611\n",
      "Epoch 288 train_loss: 0.0000232583 val_loss: 0.1050008409\n",
      "Epoch 289 train_loss: 0.0000174439 val_loss: 0.1043428563\n",
      "Epoch 290 train_loss: 0.0000292206 val_loss: 0.1052122180\n",
      "Epoch 291 train_loss: 0.0000871979 val_loss: 0.1042719331\n",
      "Epoch 292 train_loss: 0.0000231507 val_loss: 0.1049744725\n",
      "Epoch 293 train_loss: 0.0001116695 val_loss: 0.1050421330\n",
      "Epoch 294 train_loss: 0.0000205714 val_loss: 0.1045285098\n",
      "Epoch 295 train_loss: 0.0000337549 val_loss: 0.1049742525\n",
      "Epoch 296 train_loss: 0.0000656032 val_loss: 0.1034051082\n",
      "Epoch 297 train_loss: 0.0000498414 val_loss: 0.1039215693\n",
      "Epoch 298 train_loss: 0.0000434044 val_loss: 0.1039640964\n",
      "Epoch 299 train_loss: 0.0000195901 val_loss: 0.1045529207\n",
      "Epoch 300 train_loss: 0.0000163615 val_loss: 0.1043009444\n",
      "Epoch 301 train_loss: 0.0000348103 val_loss: 0.1035740377\n",
      "Epoch 302 train_loss: 0.0000611310 val_loss: 0.1046006117\n",
      "Epoch 303 train_loss: 0.0000343433 val_loss: 0.1043848448\n",
      "Epoch 304 train_loss: 0.0000368107 val_loss: 0.1056190481\n",
      "Epoch 305 train_loss: 0.0000688510 val_loss: 0.1043863783\n",
      "Epoch 306 train_loss: 0.0000526565 val_loss: 0.1041748904\n",
      "Epoch 307 train_loss: 0.0000298946 val_loss: 0.1043980253\n",
      "Epoch 308 train_loss: 0.0000400612 val_loss: 0.1041195681\n",
      "Epoch 309 train_loss: 0.0000590135 val_loss: 0.1046169616\n",
      "Epoch 310 train_loss: 0.0000224670 val_loss: 0.1049140359\n",
      "Epoch 311 train_loss: 0.0000456780 val_loss: 0.1043474336\n",
      "Epoch 312 train_loss: 0.0000732559 val_loss: 0.1039606461\n",
      "Epoch 313 train_loss: 0.0000284012 val_loss: 0.1032937879\n",
      "Epoch 314 train_loss: 0.0000402181 val_loss: 0.1041411014\n",
      "Epoch 315 train_loss: 0.0000539367 val_loss: 0.1048299435\n",
      "Epoch 316 train_loss: 0.0000424859 val_loss: 0.1043306826\n",
      "Epoch 317 train_loss: 0.0000465324 val_loss: 0.1059453271\n",
      "Epoch 318 train_loss: 0.0000199138 val_loss: 0.1040591020\n",
      "Epoch 319 train_loss: 0.0000849915 val_loss: 0.1033009284\n",
      "Epoch 320 train_loss: 0.0000270175 val_loss: 0.1047660883\n",
      "Epoch 321 train_loss: 0.0000268125 val_loss: 0.1042108740\n",
      "Epoch 322 train_loss: 0.0000966233 val_loss: 0.1032525740\n",
      "Epoch 323 train_loss: 0.0000389440 val_loss: 0.1037714454\n",
      "Epoch 324 train_loss: 0.0000226461 val_loss: 0.1045139620\n",
      "Epoch 325 train_loss: 0.0000457526 val_loss: 0.1047684805\n",
      "Epoch 326 train_loss: 0.0000148893 val_loss: 0.1036576851\n",
      "Epoch 327 train_loss: 0.0000486356 val_loss: 0.1039048943\n",
      "Epoch 328 train_loss: 0.0000587749 val_loss: 0.1035176630\n",
      "Epoch 329 train_loss: 0.0000422323 val_loss: 0.1041484852\n",
      "Epoch 330 train_loss: 0.0000239515 val_loss: 0.1048425387\n",
      "Epoch 331 train_loss: 0.0000343188 val_loss: 0.1035861200\n",
      "Epoch 332 train_loss: 0.0000325042 val_loss: 0.1037514230\n",
      "Epoch 333 train_loss: 0.0000155603 val_loss: 0.1040719202\n",
      "Epoch 334 train_loss: 0.0000528658 val_loss: 0.1046912925\n",
      "Epoch 335 train_loss: 0.0000414086 val_loss: 0.1037132440\n",
      "Epoch 336 train_loss: 0.0000738553 val_loss: 0.1045352287\n",
      "Epoch 337 train_loss: 0.0000264694 val_loss: 0.1046145202\n",
      "Epoch 338 train_loss: 0.0000164740 val_loss: 0.1043757140\n",
      "Epoch 339 train_loss: 0.0000479132 val_loss: 0.1043399670\n",
      "Epoch 340 train_loss: 0.0000142316 val_loss: 0.1034041657\n",
      "Epoch 341 train_loss: 0.0000211530 val_loss: 0.1042214594\n",
      "Epoch 342 train_loss: 0.0000269628 val_loss: 0.1041675215\n",
      "Epoch 343 train_loss: 0.0000470727 val_loss: 0.1048889308\n",
      "Epoch 344 train_loss: 0.0000533633 val_loss: 0.1045691790\n",
      "Epoch 345 train_loss: 0.0001101643 val_loss: 0.1045619607\n",
      "Epoch 346 train_loss: 0.0000331228 val_loss: 0.1046323729\n",
      "Epoch 347 train_loss: 0.0000273726 val_loss: 0.1037860913\n",
      "Epoch 348 train_loss: 0.0000683565 val_loss: 0.1042876466\n",
      "Epoch 349 train_loss: 0.0000186492 val_loss: 0.1048068350\n",
      "Epoch 350 train_loss: 0.0000352174 val_loss: 0.1046990145\n",
      "Epoch 351 train_loss: 0.0000964211 val_loss: 0.1047298816\n",
      "Epoch 352 train_loss: 0.0000318471 val_loss: 0.1042691733\n",
      "Epoch 353 train_loss: 0.0000409700 val_loss: 0.1040015200\n",
      "Epoch 354 train_loss: 0.0000667554 val_loss: 0.1042245468\n",
      "Epoch 355 train_loss: 0.0000695896 val_loss: 0.1045281078\n",
      "Epoch 356 train_loss: 0.0000844265 val_loss: 0.1046381284\n",
      "Epoch 357 train_loss: 0.0000209750 val_loss: 0.1035701882\n",
      "Epoch 358 train_loss: 0.0000599494 val_loss: 0.1050182126\n",
      "Epoch 359 train_loss: 0.0000441378 val_loss: 0.1039308643\n",
      "Epoch 360 train_loss: 0.0000518310 val_loss: 0.1042296241\n",
      "Epoch 361 train_loss: 0.0000842715 val_loss: 0.1034320878\n",
      "Epoch 362 train_loss: 0.0000471781 val_loss: 0.1040607492\n",
      "Epoch 363 train_loss: 0.0000431305 val_loss: 0.1040987895\n",
      "Epoch 364 train_loss: 0.0000594855 val_loss: 0.1045209459\n",
      "Epoch 365 train_loss: 0.0000712940 val_loss: 0.1038870447\n",
      "Epoch 366 train_loss: 0.0000418461 val_loss: 0.1037298810\n",
      "Epoch 367 train_loss: 0.0000370949 val_loss: 0.1043309002\n",
      "Epoch 368 train_loss: 0.0000578288 val_loss: 0.1035964460\n",
      "Epoch 369 train_loss: 0.0000415432 val_loss: 0.1043192549\n",
      "Epoch 370 train_loss: 0.0000450262 val_loss: 0.1039859726\n",
      "Epoch 371 train_loss: 0.0000923027 val_loss: 0.1047758590\n",
      "Epoch 372 train_loss: 0.0000127133 val_loss: 0.1044321289\n",
      "Epoch 373 train_loss: 0.0000379080 val_loss: 0.1042364368\n",
      "Epoch 374 train_loss: 0.0000667471 val_loss: 0.1038289972\n",
      "Epoch 375 train_loss: 0.0001205418 val_loss: 0.1031573722\n",
      "Epoch 376 train_loss: 0.0000381101 val_loss: 0.1040425531\n",
      "Epoch 377 train_loss: 0.0000689092 val_loss: 0.1048583640\n",
      "Epoch 378 train_loss: 0.0000507265 val_loss: 0.1049986310\n",
      "Epoch 379 train_loss: 0.0000652561 val_loss: 0.1039893957\n",
      "Epoch 380 train_loss: 0.0000351887 val_loss: 0.1040046460\n",
      "Epoch 381 train_loss: 0.0002839084 val_loss: 0.1043094643\n",
      "Epoch 382 train_loss: 0.0000169925 val_loss: 0.1044383683\n",
      "Epoch 383 train_loss: 0.0000702462 val_loss: 0.1043201749\n",
      "Epoch 384 train_loss: 0.0000318568 val_loss: 0.1049895889\n",
      "Epoch 385 train_loss: 0.0000304969 val_loss: 0.1036950323\n",
      "Epoch 386 train_loss: 0.0000183535 val_loss: 0.1044633773\n",
      "Epoch 387 train_loss: 0.0000654702 val_loss: 0.1047864469\n",
      "Epoch 388 train_loss: 0.0000648511 val_loss: 0.1035403616\n",
      "Epoch 389 train_loss: 0.0000405139 val_loss: 0.1048974255\n",
      "Epoch 390 train_loss: 0.0000403306 val_loss: 0.1040556524\n",
      "Epoch 391 train_loss: 0.0000313922 val_loss: 0.1045744294\n",
      "Epoch 392 train_loss: 0.0000842906 val_loss: 0.1031121849\n",
      "Epoch 393 train_loss: 0.0000154431 val_loss: 0.1044874338\n",
      "Epoch 394 train_loss: 0.0000802385 val_loss: 0.1035772264\n",
      "Epoch 395 train_loss: 0.0000150198 val_loss: 0.1044178545\n",
      "Epoch 396 train_loss: 0.0000263142 val_loss: 0.1036726700\n",
      "Epoch 397 train_loss: 0.0000447957 val_loss: 0.1037749786\n",
      "Epoch 398 train_loss: 0.0000144528 val_loss: 0.1044058912\n",
      "Epoch 399 train_loss: 0.0000925700 val_loss: 0.1040118468\n",
      "Epoch 400 train_loss: 0.0000544354 val_loss: 0.1041358974\n",
      "Epoch 401 train_loss: 0.0000190136 val_loss: 0.1043339880\n",
      "Epoch 402 train_loss: 0.0000298977 val_loss: 0.1041477714\n",
      "Epoch 403 train_loss: 0.0000163216 val_loss: 0.1048040084\n",
      "Epoch 404 train_loss: 0.0000257258 val_loss: 0.1052079426\n",
      "Epoch 405 train_loss: 0.0000217621 val_loss: 0.1038607091\n",
      "Epoch 406 train_loss: 0.0000326176 val_loss: 0.1056297971\n",
      "Epoch 407 train_loss: 0.0000254842 val_loss: 0.1038086598\n",
      "Epoch 408 train_loss: 0.0000656072 val_loss: 0.1042878294\n",
      "Epoch 409 train_loss: 0.0000776718 val_loss: 0.1037613197\n",
      "Epoch 410 train_loss: 0.0000718589 val_loss: 0.1031227827\n",
      "Epoch 411 train_loss: 0.0000264903 val_loss: 0.1044295559\n",
      "Epoch 412 train_loss: 0.0000223163 val_loss: 0.1042383357\n",
      "Epoch 413 train_loss: 0.0000245079 val_loss: 0.1047028817\n",
      "Epoch 414 train_loss: 0.0000267747 val_loss: 0.1048539775\n",
      "Epoch 415 train_loss: 0.0000557237 val_loss: 0.1035617156\n",
      "Epoch 416 train_loss: 0.0000147956 val_loss: 0.1049632154\n",
      "Epoch 417 train_loss: 0.0000389629 val_loss: 0.1044624918\n",
      "Epoch 418 train_loss: 0.0000206964 val_loss: 0.1041077077\n",
      "Epoch 419 train_loss: 0.0000324570 val_loss: 0.1038817837\n",
      "Epoch 420 train_loss: 0.0000268971 val_loss: 0.1049256642\n",
      "Epoch 421 train_loss: 0.0000226411 val_loss: 0.1037172980\n",
      "Epoch 422 train_loss: 0.0001233734 val_loss: 0.1040174725\n",
      "Epoch 423 train_loss: 0.0000320094 val_loss: 0.1042999431\n",
      "Epoch 424 train_loss: 0.0000160139 val_loss: 0.1035205348\n",
      "Epoch 425 train_loss: 0.0000862423 val_loss: 0.1042862136\n",
      "Epoch 426 train_loss: 0.0000363997 val_loss: 0.1041318461\n",
      "Epoch 427 train_loss: 0.0000606724 val_loss: 0.1039836938\n",
      "Epoch 428 train_loss: 0.0000796023 val_loss: 0.1040715042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429 train_loss: 0.0000392072 val_loss: 0.1046602587\n",
      "Epoch 430 train_loss: 0.0000753325 val_loss: 0.1050356907\n",
      "Epoch 431 train_loss: 0.0000720754 val_loss: 0.1049745478\n",
      "Epoch 432 train_loss: 0.0000374565 val_loss: 0.1042229474\n",
      "Epoch 433 train_loss: 0.0000423283 val_loss: 0.1051325684\n",
      "Epoch 434 train_loss: 0.0000294991 val_loss: 0.1050630281\n",
      "Epoch 435 train_loss: 0.0000326743 val_loss: 0.1050128575\n",
      "Epoch 436 train_loss: 0.0000564384 val_loss: 0.1043697401\n",
      "Epoch 437 train_loss: 0.0000537408 val_loss: 0.1043646307\n",
      "Epoch 438 train_loss: 0.0000075977 val_loss: 0.1039478623\n",
      "Epoch 439 train_loss: 0.0000791690 val_loss: 0.1048886260\n",
      "Epoch 440 train_loss: 0.0000180019 val_loss: 0.1044350173\n",
      "Epoch 441 train_loss: 0.0000318139 val_loss: 0.1049244381\n",
      "Epoch 442 train_loss: 0.0000240454 val_loss: 0.1044880050\n",
      "Epoch 443 train_loss: 0.0000417151 val_loss: 0.1038769976\n",
      "Epoch 444 train_loss: 0.0000754268 val_loss: 0.1039575735\n",
      "Epoch 445 train_loss: 0.0000364595 val_loss: 0.1042178575\n",
      "Epoch 446 train_loss: 0.0000536905 val_loss: 0.1043233909\n",
      "Epoch 447 train_loss: 0.0000355589 val_loss: 0.1041195145\n",
      "Epoch 448 train_loss: 0.0001103816 val_loss: 0.1038967091\n",
      "Epoch 449 train_loss: 0.0000115695 val_loss: 0.1041862600\n",
      "Epoch 450 train_loss: 0.0001039143 val_loss: 0.1040346855\n",
      "Epoch 451 train_loss: 0.0000304413 val_loss: 0.1036758109\n",
      "Epoch 452 train_loss: 0.0000445469 val_loss: 0.1040961600\n",
      "Epoch 453 train_loss: 0.0000229204 val_loss: 0.1047379260\n",
      "Epoch 454 train_loss: 0.0000348193 val_loss: 0.1047887772\n",
      "Epoch 455 train_loss: 0.0000353779 val_loss: 0.1040947569\n",
      "Epoch 456 train_loss: 0.0000863692 val_loss: 0.1047684700\n",
      "Epoch 457 train_loss: 0.0000613266 val_loss: 0.1048933388\n",
      "Epoch 458 train_loss: 0.0000638635 val_loss: 0.1041918045\n",
      "Epoch 459 train_loss: 0.0000332147 val_loss: 0.1045627315\n",
      "Epoch 460 train_loss: 0.0000866175 val_loss: 0.1044871162\n",
      "Epoch 461 train_loss: 0.0000362602 val_loss: 0.1050081680\n",
      "Epoch 462 train_loss: 0.0000228794 val_loss: 0.1038198796\n",
      "Epoch 463 train_loss: 0.0000351123 val_loss: 0.1055036778\n",
      "Epoch 464 train_loss: 0.0000214171 val_loss: 0.1044906706\n",
      "Epoch 465 train_loss: 0.0000228695 val_loss: 0.1040371897\n",
      "Epoch 466 train_loss: 0.0000809360 val_loss: 0.1039733267\n",
      "Epoch 467 train_loss: 0.0000421823 val_loss: 0.1037578046\n",
      "Epoch 468 train_loss: 0.0000187121 val_loss: 0.1047410310\n",
      "Epoch 469 train_loss: 0.0000608343 val_loss: 0.1039884475\n",
      "Epoch 470 train_loss: 0.0001054804 val_loss: 0.1041843104\n",
      "Epoch 471 train_loss: 0.0000294798 val_loss: 0.1040712915\n",
      "Epoch 472 train_loss: 0.0000742097 val_loss: 0.1043590771\n",
      "Epoch 473 train_loss: 0.0000344061 val_loss: 0.1049057534\n",
      "Epoch 474 train_loss: 0.0000979353 val_loss: 0.1037398538\n",
      "Epoch 475 train_loss: 0.0000187837 val_loss: 0.1047365703\n",
      "Epoch 476 train_loss: 0.0000645477 val_loss: 0.1048251296\n",
      "Epoch 477 train_loss: 0.0000177984 val_loss: 0.1041339089\n",
      "Epoch 478 train_loss: 0.0000841004 val_loss: 0.1040649529\n",
      "Epoch 479 train_loss: 0.0001009212 val_loss: 0.1043027753\n",
      "Epoch 480 train_loss: 0.0000732601 val_loss: 0.1043005527\n",
      "Epoch 481 train_loss: 0.0000180595 val_loss: 0.1036084778\n",
      "Epoch 482 train_loss: 0.0000141954 val_loss: 0.1038873072\n",
      "Epoch 483 train_loss: 0.0000242114 val_loss: 0.1035864609\n",
      "Epoch 484 train_loss: 0.0000220620 val_loss: 0.1043166074\n",
      "Epoch 485 train_loss: 0.0000502028 val_loss: 0.1046694046\n",
      "Epoch 486 train_loss: 0.0000305242 val_loss: 0.1049447303\n",
      "Epoch 487 train_loss: 0.0000327710 val_loss: 0.1050987626\n",
      "Epoch 488 train_loss: 0.0000377914 val_loss: 0.1045866746\n",
      "Epoch 489 train_loss: 0.0000363694 val_loss: 0.1044104470\n",
      "Epoch 490 train_loss: 0.0000660976 val_loss: 0.1048018496\n",
      "Epoch 491 train_loss: 0.0001401206 val_loss: 0.1045758670\n",
      "Epoch 492 train_loss: 0.0000222780 val_loss: 0.1046117696\n",
      "Epoch 493 train_loss: 0.0000199282 val_loss: 0.1040650645\n",
      "Epoch 494 train_loss: 0.0000380901 val_loss: 0.1048011490\n",
      "Epoch 495 train_loss: 0.0000868515 val_loss: 0.1045323746\n",
      "Epoch 496 train_loss: 0.0000440147 val_loss: 0.1037226042\n",
      "Epoch 497 train_loss: 0.0000100719 val_loss: 0.1043756848\n",
      "Epoch 498 train_loss: 0.0000596614 val_loss: 0.1048760983\n",
      "Epoch 499 train_loss: 0.0000202430 val_loss: 0.1042915541\n",
      "Epoch 500 train_loss: 0.0000290541 val_loss: 0.1040488601\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "        single_loss = criterion(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            y_pred = model(inputs)\n",
    "            val_loss += criterion(y_pred, labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1} train_loss: {single_loss.item():.10f} val_loss: {val_loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d7de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000408171\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model(inputs)\n",
    "        labels = labels.view_as(y_pred)\n",
    "\n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store predictions and actual values for further analysis if needed\n",
    "        predictions.append(y_pred.numpy())  # or y_pred.cpu().numpy() if using GPU\n",
    "        actuals.append(labels.numpy())\n",
    "\n",
    "# Calculate average loss over the test set\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.10f}')\n",
    "# Test Loss: 0.0000459561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2233dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.tail of                     time       low      high      open     close       volume\n",
      "0    2023-11-16 12:00:00  36260.32  37332.32  37260.42  36316.17  6840.344833\n",
      "1    2023-11-16 18:00:00  35511.11  36400.18  36314.57  36161.15  7177.317828\n",
      "2    2023-11-17 00:00:00  36097.28  36674.75  36161.15  36363.27  2676.160827\n",
      "3    2023-11-17 06:00:00  36131.00  36473.75  36365.33  36382.68  1550.020928\n",
      "4    2023-11-17 12:00:00  35869.00  36831.99  36382.69  36508.83  4979.856742\n",
      "..                   ...       ...       ...       ...       ...          ...\n",
      "151  2023-12-24 06:00:00  43434.62  43721.81  43517.85  43655.99   637.606020\n",
      "152  2023-12-24 12:00:00  43578.33  43901.70  43658.10  43654.11   882.832758\n",
      "153  2023-12-24 18:00:00  42614.17  43719.56  43656.52  43025.03  2143.310127\n",
      "154  2023-12-25 00:00:00  42755.35  43241.15  43025.02  43231.21  1245.264334\n",
      "155  2023-12-25 06:00:00  43056.87  43339.64  43233.08  43188.37   807.055913\n",
      "\n",
      "[156 rows x 6 columns]>\n",
      "Predicted next 5 closing prices: [43178.227, 43174.035, 43141.867, 43144.434, 43112.746, 43098.523, 43068.684, 43057.984, 43042.363, 43026.105, 43012.324, 42996.258, 42982.35, 42966.836, 42952.332, 42937.656]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
      "/var/folders/0x/0_w4ycb174s35r457mw7vlb40000gn/T/ipykernel_71969/3120318120.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and normalize the inference data\n",
    "inf_data = pd.read_csv(\"inf.csv\")\n",
    "print(inf_data.tail)\n",
    "\n",
    "# Fit a separate scaler for the 'close' feature\n",
    "close_scaler = MinMaxScaler()\n",
    "inf_data['close'] = close_scaler.fit_transform(inf_data[['close']])\n",
    "\n",
    "# Prepare the initial recent data for prediction\n",
    "window_size = 10  # Adjust based on your model's training\n",
    "recent_data = inf_data['close'][-window_size:].values.reshape(1, window_size, 1)  # Reshape for single batch, single feature\n",
    "\n",
    "# Recursively predict next 5 time steps\n",
    "num_predictions = 16\n",
    "predictions = []\n",
    "bi_predictions = []\n",
    "\n",
    "for _ in range(num_predictions):\n",
    "    # Convert recent data to tensor\n",
    "    recent_data_tensor = torch.FloatTensor(recent_data)\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_next_step = model(recent_data_tensor).numpy()  # Get the model's prediction for the next step\n",
    "\n",
    "    # Inverse transform to get actual price scale for the predicted step\n",
    "    predicted_close_price = close_scaler.inverse_transform(predicted_next_step.reshape(-1, 1))[0, 0]\n",
    "    predictions.append(predicted_close_price)\n",
    "\n",
    "    # Update the recent data with the predicted value\n",
    "    recent_data = np.roll(recent_data, -1, axis=1)  # Shift everything one step to the left\n",
    "    recent_data[0, -1, 0] = predicted_next_step  # Insert the prediction at the end of recent_data\n",
    "\n",
    "# predictions now contains the 5 sequential forecasted values\n",
    "print(\"Predicted next 5 closing prices:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crypto_model)",
   "language": "python",
   "name": "crypto_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
